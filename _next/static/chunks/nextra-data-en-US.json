{"/":{"title":"Introduction","data":{"":"This is the central repository for technical documentation of projects developed and maintained by Department of Innovation and Technology of the City of Bratislava. if you'd like to contribute, get in touch via email or github\nIf you are an individual or a company who’d like to take part in these efforts, collaborate closely on development or report an issue, we’d love to hear from you! 🙌 Contact us using this repository or at innovationteam@bratislava.sk\nOur goal is to be transparent about services we’re developing and providing, as well as to invite other cities and municipalities to build on top of the same or similar open-source technologies we’ve already tested and used - to foster an ecosystem of collaboration between teams facing similar challenges. We’ll be happy to get in touch.","getting-started#Getting started":"Hey! 👋 If you're a new contributor, member of the Innovations team or an external contractor, these are the pages we recommend starting with:","general#General":"Editor setup\nGit workflow\nDocker setup\nPostgres setup","frontend#Frontend":"If you're not familiar with any of these technologies:\nNextJS\nTypescript\nStrapi CMS\nGraphQL, which we use for connecting Next to Strapi\nGraphQL Codegen, which we use to generate typesafe SDK from our GraphQL queries\nMeilisearch, often connected to Strapi via meilisearch-strapi plugin\nSentry\n\nAfterwards:\nour GraphQL Codegen setup\nhow we handle environment variables","backend#Backend":"Nest.js\nTypescript\nPrisma\n\nAfterwards, you can check out the NextJS section of documentation for more details on how we use these technologies.","structure#Structure":"","recipes#Recipes":"This section serves as a \"cookbook\" for jobs and setups we need to do repeatedly. If you encounter (and struggle through) a task, steps of which were not trivial, and with which you can help with your newly gained knowledge, please write anything relevant into this section of the documentation.","deployment-and-infrastructure#Deployment and infrastructure":"Contains the documentation & information for tasks concerning deployment and maintenance of our apps on our Kubernetes clusters (or elsewhere). If you are an open-source contributor or an external contractor, you likely won't need to work with this part of the setup.","technology-name-pages#Technology-name pages":"Contain information about our use of the technology or library common across all of our projects.","project-name-pages#Project-name pages":"Contain information specific to the project.","other#Other":"Organizational, onboarding, not related to development itself, other 🙂"}},"/meilisearch":{"title":"Meilisearch","data":{"":"We use meilisearch for all kinds of searching - often simply through meilisearch-strapi-plugin (read below), but in the future also to scrape the library catalog.","meilisearch-instance-setup#Meilisearch instance setup":"Important - there might be custom index/search setup for each project - that should be covered in project-specific documentation","running-meilisearch-instance-locally#Running Meilisearch instance locally":"For local development, we often use docker compose. The easiest way to add local meilisearch instance to your app is to add it as service to docker-compose.yml file as you can see bellow.\nservices:\n...\n\nmeilisearch:\nimage: getmeili/meilisearch:v0.28\nenvironment:\n- http_proxy\n- https_proxy\n- MEILI_MASTER_KEY=YOUR_MASTER_KEY\n- MEILI_NO_ANALYTICS=true\n- MEILI_ENV=development\n- MEILI_LOG_LEVEL\n- MEILI_DB_PATH=/data.ms\nports:\n- 7700:7700\nexpose:\n- '7700'\nvolumes:\n- ./meilisearch/data.ms:/data.ms\nrestart: unless-stopped\n:::caution\nDon't forget to add meilisearch folder to your .gitignore file so it's get ignored.\n:::For more information about meilisearch, please read the official meilisearch docs.","running-the-meilisearch-instance-in-kubernetes#Running the Meilisearch instance in Kubernetes":"Note: This section needs to be verified, info might be missing or incorrect\nYou can setup everything relating to your instance using kustomize. Presently this setup tends to be clumped with strapi kustomize files, so in the paths below we're always referencing such existing strapi setup.Create the following files:kubernetes/base/service-meilisearch.yml\napiVersion: v1\nkind: Service\nmetadata:\nname: ${BUILD_REPOSITORY_NAME}-meilisearch\nnamespace: ${NAMESPACE}\nspec:\ntype: ClusterIP\nports:\n- name: internal\nport: 7700\ntargetPort: http\nprotocol: TCP\n- name: external\nport: 80\ntargetPort: 7700\nprotocol: TCP\nselector:\nservice: meilisearch\nkubernetes/base/stateful-set-meilisearch.yml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nname: ${BUILD_REPOSITORY_NAME}-meilisearch\nnamespace: ${NAMESPACE}\nlabels:\nservice: meilisearch\nspec:\nselector:\nmatchLabels:\napp: meilisearch\nreplicas: 1\nserviceName: ${BUILD_REPOSITORY_NAME}-meilisearch\ntemplate:\nmetadata:\nlabels:\nservice: meilisearch\nspec:\ncontainers:\n- name: meilisearch\nimage: \"getmeili/meilisearch:v0.28.0\"\nimagePullPolicy: IfNotPresent\nenvFrom:\n- configMapRef:\nname: ${BUILD_REPOSITORY_NAME}-meilisearch\n- secretRef:\nname: ${BUILD_REPOSITORY_NAME}-meilisearch-secret\nports:\n- name: http\ncontainerPort: 7700\nprotocol: TCP\nlivenessProbe:\nhttpGet:\npath: /health\nport: http\nperiodSeconds: 60\ninitialDelaySeconds: 60\nreadinessProbe:\nhttpGet:\npath: /health\nport: http\nperiodSeconds: 60\ninitialDelaySeconds: 60\nvolumeMounts:\n- name: ${BUILD_REPOSITORY_NAME}-meilisearch-data-storage\nmountPath: /meili_data\nresources: {}\nvolumeClaimTemplates:\n- metadata:\nname: ${BUILD_REPOSITORY_NAME}-meilisearch-data-storage\nlabels:\nservice: meilisearch\nspec:\naccessModes: [\"ReadWriteOnce\"]\nresources:\nrequests:\nstorage: 10Gi\nkubernetes/base/ingress.yml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: ${BUILD_REPOSITORY_NAME}-ingress\nnamespace: ${NAMESPACE}\nlabels:\nservice: app\nannotations:\ncert-manager.io/cluster-issuer: letsencrypt\ncert-manager.io/issue-temporary-certificate: \"true\"\nkubernetes.io/ingress.class: haproxy\nspec:\ntls:\n- hosts:\n- ${HOSTNAME}\n- ${BUILD_REPOSITORY_NAME}-meilisearch.${DEPLOYMENT_ENV}bratislava.sk\nsecretName: ${BUILD_REPOSITORY_NAME}-tls\nrules:\n- host: ${HOSTNAME}\nhttp:\npaths:\n- path: /\npathType: ImplementationSpecific\nbackend:\nservice:\nname: ${BUILD_REPOSITORY_NAME}-app\nport:\nnumber: 80\n- host: ${BUILD_REPOSITORY_NAME}-meilisearch.${DEPLOYMENT_ENV}bratislava.sk\nhttp:\npaths:\n- path: /\npathType: ImplementationSpecific\nbackend:\nservice:\nname: ${BUILD_REPOSITORY_NAME}-meilisearch\nport:\nnumber: 80\nAdd following to kubernetes/base/kustomization.yml\nresources: ...\n- ingress.yml\n- stateful-set-meilisearch.yml\n- service-meilisearch.yml\n\nconfigMapGenerator:\n- name: ${BUILD_REPOSITORY_NAME}-meilisearch\nnamespace: ${NAMESPACE}\nenvs:\n- .meilisearch.env\nAnd to deployment.yml\n...\nenvFrom:\n- secretRef:\nname: ${BUILD_REPOSITORY_NAME}-meilisearch-secret\n- configMapRef:\nname: ${BUILD_REPOSITORY_NAME}-meilisearch","environment-setup#Environment setup":"You'll want to setup a meilisearch master key as a secret (check out the section on env vars & secrets), you can see the secret reference above already. The sealed secret file will looks something like this:\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\nannotations:\nsealedsecrets.bitnami.com/namespace-wide: \"true\"\ncreationTimestamp: null\nname: <your-prefix>-meilisearch-secret\nnamespace: standalone\nspec:\nencryptedData:\nMEILI_MASTER_KEY: YOUR_BASE_64_ENCODED_KEY\ntemplate:\ndata: null\nmetadata:\nannotations:\nsealedsecrets.bitnami.com/namespace-wide: \"true\"\ncreationTimestamp: null\nname: <your-prefix>-meilisearch-secret\nnamespace: standalone\n\nThis is different from the search secret you'll be using to query meilisearch data, and should preferably be different from your admin secret as well, so that you can later swap it painlessly\nAlong with an appropriate .meilisearch.env file for non-secret setup, usually residing at kubernetes/base/.meilisearch.env\nMEILI_ENV=production\nMEILI_NO_ANALYTICS=true\nSetting MEILI_ENV to development serves a gui with search console by the meili server.In case you have also separate .meilisearch.env files for each environment, reference the correct one in kubernetes/envs/<env>/kustommization.yml\nconfigMapGenerator:\n...\n- name: ${BUILD_REPOSITORY_NAME}-meilisearch\nnamespace: ${NAMESPACE}\nbehavior: merge\nenvs:\n- .meilisearch.env","getting-the-search-and-admin-keys#Getting the search and admin keys":"Get the keys.\ncurl --request GET \\\n--url http://your-meili-url/keys \\\n--header 'Authorization: Bearer YOUR_MASTER_KEY' \\\n--header 'Content-Type: application/json' | json_pp\nSet admin key and host in Strapi meilisearch plugin settings, e.g. bratislava-strapi-meilisearch:7700 for host.Add search key an host to Next env variables.More info in meilisearch docs.","setting-up-new-index#Setting up new index":"Before you can search your data, you need to index them.","manual-indexing#Manual indexing":"For manual indexing, follow the Meilisearch docs. We don't really do this here...","connecting-strapi-to-meilisearch#Connecting Strapi to Meilisearch":"Today our Strapi indexes are setup and filled with GUI using meilisearch-strapi-plugin.The original plugin deals only with the primary locale. A one-liner change makes it index all of the locales, but these will then be clumped together. Because of that, we are using our own fork of meilisearch-strapi-plugin.You can install it using:\nyarn add https://github.com/bratislava/strapi-plugin-meilisearch.git\nThen you can add your configuration into config/plugins.ts or config/plugins.js.\nexport default {\n...\nmeilisearch: {\nconfig: {\nhost: process.env.MEILISEARCH_HOST,\napiKey: process.env.MEILISEARCH_ADMIN_API_KEY,\n},\n},\n}\n:::note\nEvery project has it's own environment handling (which is often slightly different between projects), so make sure you have your variables set properly.\n:::","localization-solution#Localization solution":"For every strapi collection that is both localized and should be searchable you have to set locale as filterableAttribute.Here is an example for event collection:\nexport default {\n...\nmeilisearch: {\nconfig: {\n...\nevent: {\nsettings: {\nfilterableAttributes: ['locale'],\n},\n},\n},\n},\n}\nAfterwards, after Strapi restart and rehooking updated collections in UI you can filter by locale so that your results don't get mixed up, like so:\ncurl --request POST \\\n--url https://your-meili-url/indexes/your-index-name/search \\\n--header 'Authorization: Bearer YOUR_SEARCH_TOKEN' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"q\": \"Youth\",\n\"filter\": [\"locale = en\"]\n}'\nMore info in meilisearch docs.","additional-settings#Additional settings":"Just like with filterableAttributes, you can further configure your indexes (i.e. add sortableAttributes) - check out meilisearch docs.","accessing-search-functionality#Accessing search functionality":"When you have your indexes setup (using strapi plugin or manually), you can search them using the following\ndebug using either Postman or by accessing the dev gui served by the meilisearch instance in dev mode\nfrontend/backend using the meilisearch-js sdk\n\nCheck out Meilisearch pages/github before delving into custom integrations, as they provide & maintain, including embedable search bars and scrapers."}},"/graphql":{"title":"GraphQL","data":{"":"We're using GraphQL priarily when talking to our Strapi CMS servers. Read more about getting the Strapi data with GraphQL in general in their docs.\nIf you are joining an existing project and not setting up a new one, you can jump ahead to Generating and using queries","connecting-to-graphql-apis-using-graphql-codegen#Connecting to GraphQL APIs using GraphQL Codegen":"We're using GraphQL Codegen setup to generate typed clients for our gql servers - particularly Strapi CMS.We're using graphql-request plugin:::note Note on using graphql-request: at the time of writing there is also apollo-next plugin - the reason we use graphql-request, which is more barebones, is because it did not exist when we were setting this up initially. Apollo-next may be worth a shot with some future project","project-setup#Project setup":"Decide where your .graphql files will live, and where your client & types should be generated. Usually we use /graphql/index.ts for the client and /graphql/queries/**/*.graphql for both queries and mutations. With this setup, our codegen.yml in our frontend (i.e. Nextjs) root looks like this:\nschema: \"http://localhost:1337/graphql\"\ndocuments: \"./graphql/queries/**/*.{gql,graphql}\"\ngenerates:\ngraphql/index.ts:\nplugins:\n- typescript\n- typescript-operations\n- typescript-graphql-request\nThis means we're generating schema against a locally running strapi server.\nThis implies we need a running local server to be able to regenerate our gql client - at the time of writing, strapi v4 does not have an easy way to export gql schema. We can use this approach once hey fix it.\nAdd the following line into package.json:\n\"scripts\": {\n...\n\"gen\": \"graphql-codegen\"\n},\nand install the following dependencies, both dev and nondev:\nyarn add graphql graphql-tag graphql-request\nyarn add -D @graphql-codegen/cli @graphql-codegen/typescript @graphql-codegen/typescript-graphql-request @graphql-codegen/typescript-operations\nTo use the generated client (sdk) against your graphql server, you need to initialize it, passing in the server endpoint as a parameter. We usually setup a file like utils/gql.ts from which we export .the typed client itself The file below deals with different formats of urls being provided from within Kubernetes deployment and local development.\nimport { getSdk } from \"../graphql/index\"\nimport { GraphQLClient } from \"graphql-request\"\nimport getConfig from \"next/config\"\n\nconst { serverRuntimeConfig } = getConfig()\n\n// URL becomes full url to strapi on server, but just /graphql (for proxy) on client\n// all the dealings with protocol are here because we route to a local service address from within k8s and to a full https:// url from local development\n\nconst protocol =\nprocess.env.STRAPI_URL &&\n(process.env.STRAPI_URL.startsWith(\"http://\") || process.env.STRAPI_URL.startsWith(\"https://\"))\n? \"\"\n: \"http://\"\nconst gql = new GraphQLClient(\n`${\nprocess.env.STRAPI_URL ? `${protocol}${serverRuntimeConfig.strapiUrl}` : window.location.origin\n}/graphql`\n)\nexport const client = getSdk(gql)","generating-and-using-queries#Generating and using queries":"You need at least a single valid query among your graphql files for client to generate correctly. Check out Exploring GraphQL Schema to find a valid query for your server. Since most of our Strapi V4 instances have a 'pages' model, the following query to get the total amount of pages usually works:\nquery TotalPages {\npages {\nmeta {\npagination {\ntotal\n}\n}\n}\n}\nYou can put this query into a *.graphql file into /graphql/queries directory.Afterwards, you should be able to run\nyarn gen\nIf everything was setup correctly, this will generate graphql/index.ts file with valid types for your schema, as well as a client with a single query - client.TotalPages","using-the-generated-client#Using the generated client":"Continuing from previous example, if you import which will will return the following data:\nimport { client } from \"../utils/gql\"\n\n// ...\n\nconst data = await client.TotalPages()\n\n/*\nif successful, data will look as follows:\ndata = {\npages {\nmeta {\npagination {\ntotal: 358 // the actual number will differ\n}\n}\n}\n}\n*/\nIn next.js, you often do this inside getStaticProps or getServerSideProps:\nexport const getStaticProps: GetStaticProps = async (ctx) => {\n// ...\nconst { pages } = await client.TotalPages()\nreturn {\nprops: {\ntotalPages: pages.meta.pagination.total,\n},\n}\n}\nIf you do this from frontend (loading data from within a React component), you likely want to wrap the client call in something like SWR, React-Query, or just useEffect. With SWR it looks like this:\nconst MyComponent = () => {\nconst { data, error } = useSWR(\"TotalPagesQuery\", () => client.TotalPages())\nconst isLoading = !data && !error\nreturn (\n<div>\nTotal pages count:{\" \"}\n{isLoading ? \"Loading\" : error ? \"Error!\" : data.pages.meta.pagination.total}\n</div>\n)\n}\nRead more in SWR/ReactQuery docs on handling params and other.","error-handling#Error handling":"If there's error on any node of the result, the whole request will throw. You likely want it wrapped in a try-catch block.This can be dealt with differently (and often should be - so that one missing piece of data does not break your whole page) - read more in the docs of the graphql-request and the codegen graphql-request plugin linked above.","exploring-graphql-schema#Exploring GraphQL schema":"You can use our fork of the OneGraph GraphiQL Explorer to browse any GraphQL endpoint. In our case it's usually an endpoint of a Strapi instance. The only thing our fork adds is easy configuration of the endpoint via .env.A quick example exploring SpaceX data hereYou can use the left panel to easily explore through all of the options provided by the api, and use the params to control filtering/sorting/pagination."}},"/bratislava.sk/add-new-component-to-sections":{"title":"Add New Component to Sections","data":{"":"These are the steps for adding a newly designed reusable visual component to the application - both as a Strapi component usable in Page and BlogPost sections and as a React Component in next codebase.A higher level overview for this can be found in our Strapi docs or in official Strapi docs.\nATTENTION Currently you always have to add the Strapi component to both BlogPost and Page sections. Otherwise you'll see an error like this: Cannot read properties of undefined (reading 'type') when you run yarn gen\n\ngo to this location in strapi\nhttp://localhost:1337/admin/plugins/content-type-builder/content-types\nthere you can see the COMPONENTS section at the bottom of the sidebar\nclick on 'add new component' button. If you can't see it verify you are running Strapi server locally and in dev mode. In this guide we are assuming you are creating a new \"Section\" - thus, add the component under the Sections category.\n\n\n\n\ncreate a component based on your data type. For example, if you want to create a component for a list of documents, you can create a component with the following fields:\ntitle - text\ndocuments - relation to Document content-type\n\n\nafter creating the component add the component into the collection-type located in sidebar aswell, as shown below in image (taken from v3 Strapi, should be similar in v4).\n\n\n\n\nyou can use GraphiQL to get the exact format of your query - check out the graphql docs\ncontinuing with the example above, assuming your component is named DocumentsList you would add something like this the Section fragment definition (inside next/graphql/queries/Pages.graphql):\n\n\nfragment Sections on PageSectionsDynamicZone {\n__typename\n\n# ... other sections\n\n... on ComponentSectionsDocumentList {\nid\ntitle\ndocuments {\nid\ntitle\nslug\ndescription\ndocument {\nid\nurl\nalternativeText\ncaption\nwidth\nheight\nformats\n}\n}\n}\n}\n\nregenerate your graphql client & types by running yarn gen in the next directory of the project - more info once again in the graphql docs\nyour data should be available on the gql client and typed correctly\n\n\nadd a new case to the switch clause in Sections.tsx, matching the __typename of your newly created component - typescript will understand this, and in the body of this clause you should be able to safely access all the fields of your Strapi component\nfor a better example of this, check out either the Sections.tsx file directly within the bratislava.sk repository or look into our Strapi general concepts docs."}},"/bratislava.sk/colors-and-themes":{"title":"Colors and Themes","data":{"":"TODO docs here once cleared up in code"}},"/bratislava.sk/forms-general":{"title":"Forms General","data":{"":"🔧 This section, as well as the feature itself, is a work-in-progress.\nWe are using react-jsonschema-form with our custom form components as widgets to build our form system.We use AJV for validation (and so does react-jsonschema-form).Read more about JSON Schema.","basic-architecture-of-frontend-eforms#Basic ‘architecture’ of frontend eForms:":"base path for code is /next in codebase\nall form definitions, files and configs are exported from /backed/forms/index.ts\neverything is rendered in page/[eform].tsx - the eform param should match the key exported from backend/forms/index\nall rjsf config (i.e. custom components to be rendered as input fields) are linked & configured in components/forms/ThemedForm.tsx\nall the logic between the steps (storing & persisting state, validating & submitting) should concentrate in useFormStepper hook in utils/forms.ts\nall the forms are submited to pages/api/forms/[id]/submit.ts - where id matches the key exported from backend/forms/index . Validation (json & xml), transformation, and forwarding to our forms BE service lives in here\nany supporting functionality should be ‘linked’ to ine of these entry points","adding-new-eform#Adding new eForm":"Create a JSON schema. RJSF playground  may be useful.\nInstall @bratislava/json-schema-xsd-tools globally - run yarn global add @bratislava/json-schema-xsd-tools or npm i -g @bratislava/json-schema-xsd-tools\nRun command json-schema-xsd-tools generate -j <json-path> -o <out>, where json-path is path to prepared JSON schema and out is eForm name (default form).\nFile <out>.ts and folder <out> are generated. Output folder includes XSD schema, stylesheets (for text, html and pdf transformations), xml template and some mock data. Copy these files into /backend/forms in bratislava.sk. Also update /backed/forms/index.ts.\nRun tests - run yarn test or npm run test. If you would like to exclude form from tests (not recommended), include form name to excludeKeys in __tests__/forms.test.ts","developing-custom-form-components#Developing custom form components":"react-jsonschema-form let's you extend the theme, as well as specify custom widgets and templates.Since we have our own design system for forms, we prefer overriding all of the components (widgets) using the theme setup. Our custom - themed - form component can be found in components/forms/ThemedForm.tsx. You can see it used on pages/forms/[eform].tsx.You can find list of widgets, fields and templates to override in the react-jsonschema-form docs linked above.","how-to-create-new-rjsf-component#How to create new RJSF component":"Create new component in /components/forms/ - if your new component is composed out of multiple components, create a folder for all of these components. It's important to keep your component controlled by value and onChange event.\nAdd your new component to styleguide, to show how it works. You should create component ShowCase in /components/styleguide/showcases/ where you will show all possible versions of the component wrapped in Wrapper and Stack. Then you will use ShowCase in /pages/styleguide.\nTo use correctly your component in RJSF, I would encourage you to create a widget wrapper (where you will use component) in /components/forms/widget-wrappers/ for multiple reasons.\nFirst reason to use a widget wrapper is the usage of custom props in your component. All RJSF widgets have these props available to use. But we may want to use some more custom props. They can be passed in options object in prop. You should read all custom props from options in the widget wrapper and pass them to the original component.\nSecond reason to use a widget wrapper is handling of onChange and value types. Some components like DatePicker, Select, or Upload may have not primitive types like string, number or array, etc., which are needed in RJSF. It's necessary to handle value and onChange types, to be compatible with the original component.\nMap your widget component into schema. In /components/forms/ThemedForm.tsx add it in object theme inside widgets property. You can also choose a special name for the component in RJSF.\nNow we can use components in any RJSF form. For testing purposes, we can use it in /backend/forms/test. We can reach this form in a browser at URL /forms/test.\nAdd a new form widget in schema.json. There is a list of all form steps. It's better to create a new step where you can create multiple form widgets which will use your custom component. You can read how to add the custom component in schema.json at RJSF docs. You need to choose value type and optionally also the title and which form widgets will be required.\nDefine which custom component will be used in uiSchema.json. Do not forget, you must define form widgets exactly like they are nested in schema.json. It's possible to choose which custom component will be used in the form for chosen widget through \"ui:widget\" with a name you choose when you mapped your component in ThemedForm. If you want to forbid the default label, you can set \"ui:label\" to false. It's possible to set here multiple component props. All custom props, which are not defined by RJSF, is possible to set through \"ui:options\". You will find more about uiSchema again in RJSF docs.\nWhen finished, add a description of the widget to the docs here, to know how to set correctly uiSchema."}},"/bratislava.sk/forms-uischema":{"title":"Forms Uischema","data":{"":"In this part we will introduce how to use all created custom widgets in RJSF. At first, we must create form widget in schema.json and then set most of the props in uiSchema.json, as we are describing this in previous chapter.\nIf you want to disable default RJSF label, you can set \"ui:label\" to false in uiSchema.json.","inputfield#InputField":"","schemajson#schema.json":"\"inputExample\": {\n\"title\": \"Input field example\",\n\"type\": \"string\",\n\"default\": \"You can write default value here\"\n}","uischemajson#uiSchema.json":"You can choose one of 4 types of leftIcon parameter. Also choose one of input types like text, password, mail, .etc.\n\"inputExample\": {\n\"ui:widget\": \"InputField\",\n\"ui:placeholder\": \"You can write placeholder here\",\n\"ui:options\": {\n\"resetIcon\": true,\n\"leftIcon\": \"person\" | \"mail\" | \"call\" | \"lock\",\n\"type\": \"text\",\n\"description\": \"This is description text\",\n\"tooltip\": \"This is tooltip text\",\n\"explicitOptional\": false,\n\"className\": \"px-2 py-4\"\n}\n}","textarea#TextArea":"","schemajson-1#schema.json":"\"textAreaExample\": {\n\"title\": \"Text area example\",\n\"type\": \"string\",\n\"default\": \"You can write default value here\"\n}","uischemajson-1#uiSchema.json":"\"textAreaExample\": {\n\"ui:widget\": \"TextArea\",\n\"ui:placeholder\": \"You can write placeholder here\",\n\"ui:options\": {\n\"description\": \"This is description text\",\n\"tooltip\": \"This is tooltip text\",\n\"explicitOptional\": false,\n\"className\": \"px-2 py-4\"\n}\n}","radiobutton#RadioButton":"","schemajson-2#schema.json":"\"radioButton\": {\n\"type\": \"string\",\n\"title\": \"Radio buttons\",\n\"oneOf\": [\n{\n\"const\": \"screen\",\n\"title\": \"Screen\",\n\"tooltip\": \"This is some tooltip1\"\n},\n{\n\"const\": \"multiply\",\n\"title\": \"Multiply\",\n\"error\": true\n},\n{\n\"const\": \"overlay\",\n\"title\": \"Overlay\"\n}\n]\n}","uischemajson-2#uiSchema.json":"You can choose one of 3 types of parameter variant.\n\"radioButton\": {\n\"ui:widget\": \"RadioButton\",\n\"ui:options\": {\n\"className\": \"flex flex-col gap-4\",\n\"variant\": \"basic\" | \"boxed\" | \"card\"\n},\n\"ui:value\": \"overlay\"\n}","selectfield#SelectField":"","schemajson---one-choice-with-string#schema.json - one choice with string":"In this type of creating options, const represents value and title represents label.\n\"oneChoiceSelect\": {\n\"type\": \"string\",\n\"title\": \"One Choice Select\",\n\"oneOf\": [\n{\"const\": \"STU FEI\", \"title\": \"fakulta elektrotechniky a informatiky\"},\n{\"const\": \"STU FCHPT\"},\n{\"const\": \"STU FIIT\", \"title\": \"fakulta informatiky a informacnych technologii\"}\n]\n}","schemajson---deprecated-one-choice-with-number#schema.json - deprecated one choice with number":"\"deprecatedOneChoiceSelect\": {\n\"type\": \"number\",\n\"title\": \"Deprecated One Choice Select\",\n\"uniqueItems\": true,\n\"enum\": [1, 2, 3, 4, 5, 6],\n\"enumNames\": [\"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\"]\n}","schemajson---multi-choice#schema.json - multi choice":"\"multiChoiceSelect\": {\n\"type\": \"array\",\n\"title\": \"Reached diplomas\",\n\"uniqueItems\": true,\n\"items\": {\n\"type\": \"string\",\n\"oneOf\": [\n{\"const\": \"STU FEI\", \"title\": \"fakulta elektrotechniky a informatiky\"},\n{\"const\": \"STU FCHPT\"},\n{\"const\": \"STU FIIT\", \"title\": \"fakulta informatiky a informacnych technologii\"}\n]\n}\n}","uischemajson-3#uiSchema.json":"\"multiChoiceSelect\": {\n\"ui:widget\": \"SelectField\",\n\"ui:placeholder\": \"You can write placeholder here\",\n\"ui:options\": {\n\"dropdownDivider\": true,\n\"selectAllOption\": true,\n\"description\": \"This is descriptiont text\",\n\"tooltip\": \"This is tooltip text\",\n\"explicitOptional\": false,\n\"className\": \"px-2 py-4\"\n}\n}","upload#Upload":"","schemajson---one-file#schema.json - one file":"\"importOneFile\": {\n\"type\": \"string\"\n}","schemajson---multi-files#schema.json - multi files":"\"importButtonMultipleFiles\": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"string\"\n}\n}","uischemajson-4#uiSchema.json":"You can choose if you want button or drag&drop Upload component by option type.\n\"importButton\": {\n\"ui:widget\": \"Upload\",\n\"ui:options\": {\n\"type\": \"button\" | \"dragAndDrop\",\n\"size\": 5,\n\"accept\": \".jpg,.pdf\",\n\"className\": \"px-2 py-4\"\n}\n}"}},"/bratislava.sk/forms-upload-file":{"title":"Forms Upload File","data":{"":"For purposes of uploading a file in the form, we are using the widget Upload. This widget has 2 types: button and drag&drop.\nUpload widget can upload one or more files. We can set these settings in uiSchema, also with the restriction of accepted file extensions and size of the file.","upload-file-on-frontend#Upload file on frontend":"When we choose one or multiple files, at first they are validated. We must check if their size is up to set maximum file size.\nIn case we dropped files, we must also check if their extension is accepted. If the file is validated without errors,\nit's sent to the server by endpoint /api/eforms/upload-file and added to the list of files under the widget with a loading spinner.\nIf there are some frontend errors, the file is not added to the file list under the widget but info about an error is written under the widget.The file name will be also changed before sending it to the server. Timestamp and random hash are added as prefixes,\ndivided by character _ between timestamp, random hash, and original name. We can also describe it by this format\ntimestamp_randomHash_originalFileName.fileExtension. The file will be saved on the server with this name.","upload-file-on-the-server#Upload file on the server":"After frontend validation, every file is separately sent to endpoint /api/eforms/upload-file by Rest API post request.\nWe use S3 Bucket to save files on the server. For Node implementation, we use the library minio.\nAt this moment, we don't have implemented S3 Bucket on our server, so we use minio playground.\nIt's possible to create public buckets here. Public login uses accessKey as username and secretKey as password.\nYou can find both at /backend/utils/minio-client.ts. When our own S3 bucket on the server will be prepared, it's just\nnecessary to change info in minioClient inside that file.Before uploading of a file, the Minio client is testing if our S3 bucket exists. If not, it would create new. When the Minio client is sure,\nthat bucket exists, it will upload and save files inside. The endpoint is returning JSON with info about success or failure.\nIf any error occurred on the server, the file in the list of files under the Upload widget on frontend will be marked with red color.","delete-file#Delete file":"It's possible to delete the uploaded files by clicking on the trash icon next to the file in the list of files under the Upload widget on frontend.\nRest API delete request is called at endpoint /api/eforms/delete-file. The server will test if S3 bucket exists and if not,\nit will throw an error because there is no reason to create any bucket, when we want to delete something we think it's already created.\nIf a bucket exists, Minio client deletes the file from our bucket.If we are using Upload with a possibility to upload only one file, we are also deleting already uploaded file if we would upload new one.\nSame API endpoint is called and after the old uploaded file will be deleted, a new one will be uploaded with another endpoint.\nBasically, it's replacing old uploaded file with a new one.","usage-in-rjsf#Usage in RJSF":"Upload widget is possible to fully use in RJSF forms. This widget is returning string or array of strings.\nThis string represents name of uploaded file. Files with error are not returned."}},"/bratislava.sk/meilisearch-setup":{"title":"Meilisearch Setup","data":{"":"If you are looking to run a Meilisearch instance locally look to the Meilisearch part of the docs. If you don't need to change the search configuration and need the search results only to work on the frontend code, connecting to staging meilisearch instance - https://bratislava-strapi-meilisearch.staging.bratislava.sk - works reasonably well (beware, though, that this will not reflect your local data if used alongside local Strapi instance).On each of our instances the following manual index setup is required - this needs to be done after both the Strapi and Meilisearch instances are up and running. Read more on this topic in the Meilisearch section of the docs.The following content types need to be indexed in the meilisearch-strapi-plugin: blog-post, page, vzn","meilisearch-master_key-location#Meilisearch MASTER_KEY location":"Find {meilisearch-secret-name} which can be found in project folder strapi -> kubernetes -> base -> secrets -> {meilisearch-file-name}.secret.{env}.yml -> metadata -> name in our case \"bratislava-strapi-meilisearch-secret\"Log in to k8 and open Lense and head to Config -> Secrets -> find {meili-secret-name} in our case \"bratislava-strapi-meilisearch-secret\"","index-settings#Index settings":"Additionaly the following index setup is required (replace the instance and the MASTER_KEY as needed, the instance below is for production meilisearch server):\ncurl --request PATCH \\\n--url https://bratislava-strapi-meilisearch.bratislava.sk/indexes/vzn/settings \\\n--header 'Authorization: Bearer MASTER_KEY' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"sortableAttributes\": [\"title\",\"validFrom\", \"publishedAt\"]\n}'\n\ncurl --request PATCH \\\n--url https://bratislava-strapi-meilisearch.bratislava.sk/indexes/blog-post/settings \\\n--header 'Authorization: Bearer MASTER_KEY' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"sortableAttributes\": [\"title\", \"publishedAt\"]\n}'\n\ncurl --request PATCH \\\n--url https://bratislava-strapi-meilisearch.bratislava.sk/indexes/blog-post/settings \\\n--header 'Authorization: Bearer MASTER_KEY' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"filterableAttributes\": [\"locale\"]\n}'\n\ncurl --request PATCH \\\n--url https://bratislava-strapi-meilisearch.bratislava.sk/indexes/page/settings \\\n--header 'Authorization: Bearer MASTER_KEY' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"filterableAttributes\": [\"locale\"]\n}'"}},"/bratislava.sk/react-component-structure":{"title":"React Component Structure","data":{"":"TODO documented here once cleared up in code"}},"/deployment-and-infrastructure/backups":{"title":"Backups","data":{"":"For backups we are using standalone installation of Velero with restic on our Tanzu kubernetes clusters (info).","description#Description":"Velero creates a custom resource velero.io on kubernetes cluster, where it's stores all of it information about backups, schedules, restores, backup locations, etc. We are using S3 buckets as a storage, which were configured during installation and can be found in \"velero.io/BackupStorageLocation\" and can be listed by running\nkubectl get backupstoragelocation --all-namespaces\nCurrently, we are running daily backups of only most important infrastructure, but this is subject to change in the future. All cluster with Velero installed and correctly configured backup locations can access all backups throughout all clusters. Meaning you can make a backup on \"prod\" cluster and restore it on \"dev\". This is possible since we are using restic to actually make backups of files and persistent volumes. Restic uses generic format to store data and therefore our backups should be portable between cluster, different infrastructures or kubernetes version. This allows us, in case of emergency, to spin up an emergency cluster and restore entire infrastructure if needed.The critical services are labeled critical=true, which you can use as selector when restoring a backups.","installation#Installation":"To use Velero, you need to install it's CLI client, which can be found here.\nTo validate if Velero is correctly install, you can run\nvelero help\nwhich should list all available commands. By default velero will use your current kubeconfig and current-context cluster. To check if everything is working with our infrastructure you can run\nvelero get backups\nwhich no matter what cluster you are connected to, should list the same backups.As we are using S3 as backup locations, it is advisable to also install tools, that can work with S3 bucket, such as\naws cli\ns5cmd\nS3 browser","usage#Usage":"Here is a non exhaustive list of examples, how to work with Velero","-to-make-a-backup-with-velero#💾 To make a backup with Velero":"velero backup create app-namespace-backup-$(date -I) --include-namespaces my-app-namespace\nwhich will create a backup of entire namespace my-app-namespace with name \"app-namespace-backup-2022-01-01\" (if it would be ran on date 2022-01-01).","-to-make-repeating-backup-with-velero#🕛 To make repeating backup with Velero":"you can create schedule\nvelero schedule create app-namespace-backup --include-namespaces my-app-namespace --schedule=\"0 0 * * *\"\nwhich will create a daily backup, each day at 00:00AM, of my-app-namespace kubernetes namespace.","-to-restore-backup-with-velero#⏪ To restore backup with Velero":"velero restore create my-app-restore --from-backup app-namespace-backup\nor restore from schedule\nvelero restore create my-app-restore --from-schedule app-namespace-backup\nwhich will restore the latest backup made from given schedule.\nYou can also use standard kubectl syntax and specify selector when creating a backup, restore, etc. For example, to restore only critical services from \"backup123\"\nvelero restore create --from-backup backup-123 --selector=critical=true\nMore information and examples can be found in\nVMWare Tanzu documentation\nOfficial Velero documentation"}},"/deployment-and-infrastructure/database-backups":{"title":"Database Backups","data":{"":"We have a db-backup pipeline available on our Azure Devops project which dumps all the postgres instances running on all of our clusters 3 times a day.You need access to the Azure Devops project to be able to view these.For guide on loading dumps like this locally (i.e. for Strapi CMS) see the appropriate recipe."}},"/deployment-and-infrastructure/deployment":{"title":"Deployment","data":{"":"Most of our projects are set up to be dockerized and deployed into our Kubernetes infrastructure. This can be done semi-manually using out bratiska-cli tool, or can be setup using github pipelines.","deploy-using-pipelines#Deploy using pipelines":"TODO","deploy-using-bratiska-cli#Deploy using bratiska-cli":"You can find detailed manual & requirements in the bratiska-cli README. TLDR:\nmake sure you are signed into docker harbor\nmake sure your are signed into the correct kbs cluster\nrun bratiska-cli deploy --<env> where <env> is one of dev, staging, prod","harbor-policies#Harbor Policies":"As we have a somewhat limited space in our Harbor instance, all repositories are subjected to retention policy.\nCurrently, every project is entitled to 30 images altogether, split across different tags:\n5 latest images with prod* tag\n5 latest images with stag* tag\n10 latest images with dev* tag\n10 latest images with some tag\nImages without any tags will be removed\n\nThe policy is ran once a day at midnight, so during the day you might push more then specified limits. Also, please note that bratiska-cli will automatically add dev-latest, stage-latest or prod-latest tags, so be careful when running production or staging pushes not to remove currently deployed production images."}},"/deployment-and-infrastructure/grafana":{"title":"Grafana","data":{"":"In this section we will provide a bit of context on our monitoring infrastructure together with information about our alerting setup, through our Grafana.","access#Access":"Grafana is available only to whitelisted people. If you need you access, please ask an administrator. If you have an access rights you can add new person in our Azure Portal.If you need to add an external developer, you can Invite them as guests into Bratislava Active Directory (Azure portal -> Active Directory -> Users -> New/Invite -> Guest/External).","a-bit-of-context#A Bit of Context":"For our monitoring and observability we use Grafana with Prometheus, Loki and Infinity plugin/application stack.You can read more in their linked documentations, but to describe the setup in short:\nGrafana is only a visualization tool with alerting capabilities. You can add additional plugins and application to it to extend it's functionality\nPrometheus is a monitoring tool that sits on top of our kubernetes infrastructure and provides various metrics (exposed through /metric endpoint) about nodes health, resources, and application (pod) state, resources, etc.\nLoki is a Grafana application that specializes in logs monitoring and alerting. We use promtail to push application logs into Loki.\nInfinity is a very simple grafana plugin, that provides HTTP requests capabilities, it can be used to monitor health endpoint and is capable of parsing JSON responses and alerting on them\n\nAll of these application can be use to monitor your application and alert in case of any issues.\nCurrently, we use this to monitor and observe our kubernetes infrastructure, together with some critical applications. For example, we are monitoring hardware resources for all our nodes and pods and when they reach critical usage >=95%, we alert on it.","dashboards#Dashboards":"If you have access to Grafana you can take a look at our dashboards, that provide more information about the state of our infrastructure and individual applications together with their logs.","pod-dashboard#Pod Dashboard":"Pod Dashboard is a application dashboard where you can see logs of applications together with their volume and system statistics.Dashboard is driven by filters that can go up to container granularity.\nYou can search through the logs with regex pattern. Log volume is color coded base on the stream it was emitted to (stdout/stderr).\nIt also provides current running status of all associated containers\nApplication system statics, such as, memory, CPU, network and disk (PVC) usage\nAt the bottom of the dashboard we have alerting panel where you can see all the alerting rules associated with your filter selection together with their state","persistent-volumes-dashboard#Persistent Volumes Dashboard":"Persistent Volumes Dashboard  is monitoring on kubernetes PV/PVC disk usage.Dashboard is driven by filters that can go up to application granularity.\nIt has current volume usage with \"standard\" gradient color coding from green to red, when the usage start hitting ~60%\nIt also provides a simple table showcasing the full volume capacity\nAnd lastly, it shows historical disk usage in % of full capacity","health-status#Health Status":"Health Status Dashboard is complex monitoring dashboard, where you can find everything from monitoring single application state with their logs all the way up to kubernetes node resource utilization.Dashboard is driven by filters that can go up to individual POD granularity.\nIt provides statistics on health status of all the application running within the cluster and their listing\nYou can also find there POD's system resource (CPU, Memory, ...) current and historical usage and running state\nContainers's system resource (CPU, Memory, ...) current and historical usage and running state\nIt also has information about application replicas\nAnd lastly, also has resource (CPU, Memory, Disk) and health information on kubernetes cluster nodes","alerting#Alerting":"For actual alerting we have setup:\nA Grafana Bratislava Slack application/bot, that you can add to your channel\nEmail address grafana[at]devops.bratislava.sk, that you can use to send a alert notification to you mailbox\n\nTo setup a new contact point, for instance if you and only you want get some specific alert, please follow our \"Add New Contact Point\" recipe.For recipes on how to create your own alert take a look at the following\nAlerting on application system resources (CPU, Memory, Disk, etc.) ⇲\nAlerting on application's logs and specific keywords or pattern in those logs ⇲\nAlerting on availability of specific endpoints or data provided by those endpoints ⇲"}},"/nestjs/Authentication":{"title":"Authentication","data":{"":"This module is for authentication by Azure B2B, but you can use it similar with different auths services.","installation#Installation":"npm install @nestjs/passport for azure-ad auth install alsonpm install passport-azure-ad'"}},"/nestjs/CardWebPay":{"title":"Cardwebpay","data":{"":"","gp-webpay#GP webpay":"Create secret private key from GP webpay and dowlnoad it also download public key. Documentation is here\nCreate new service into services/vendors/gp-webpay.tsExample:\n// TODO rewrite this in a way where it can be shared between projects (that's way I'm not making it a service for now)\n\nimport * as formurlencoded from 'form-urlencoded';\nimport { promises, readFileSync } from 'fs';\nimport { createSign, createVerify } from 'crypto';\n\nconst ALGORITHM = 'SHA1';\nconst SIGNATURE_FORMAT = 'base64';\n\nexport enum PAYMENT_OPERATION {\nCREATE_ORDER = 'CREATE_ORDER', // Card payment\nCARD_VERIFICATION = 'CARD_VERIFICATION', // Card verification\nFINALIZE_ORDER = 'FINALIZE_ORDER', // MasterPass digital wallet\n}\n\n// TODO for this to work with deployment, it either needs to be configured using env or done otherwise sensibly\n// if we want to store prod keys elsewhere change how this is handled\nconst GP_WEBPAY_PRIVATE_KEY_PATH = `./apps/nest/parking-backend/src/app/public/webpay-keys/${process.env.GP_WEBPAY_PRIVATE_KEY_FILE_PVK}`;\nconst GP_WEBPAY_PUBLIC_KEY_PATH = `./apps/nest/parking-backend/src/app/public/webpay-keys/${process.env.GP_WEBPAY_PRIVATE_KEY_FILE_PUB}`;\n\nconst createSignature = (\ndata: string,\nprivateKey: Buffer,\nprivateKeyPassphrase: string\n): string => {\nconst signer = createSign(ALGORITHM);\nsigner.update(data);\nsigner.end();\nreturn signer.sign(\n{ key: privateKey, passphrase: privateKeyPassphrase },\nSIGNATURE_FORMAT\n);\n};\n\nconst verifySignature = (\ndata: string,\nsignature: string,\npublicKey: Buffer\n): boolean => {\nconst verifier = createVerify(ALGORITHM);\nverifier.update(data);\nverifier.end();\nconst result = verifier.verify(publicKey, signature, SIGNATURE_FORMAT);\n// console.log(`Signature verified: ${result}`)\nreturn result;\n};\n\ntype GPWebpayHttpRequest = {\nMERCHANTNUMBER: string;\nOPERATION: string;\nORDERNUMBER: number;\nAMOUNT: number;\nCURRENCY?: number;\nDEPOSITFLAG: number;\nMERORDERNUM?: number;\nURL: string;\nDESCRIPTION?: string;\nMD?: string;\nUSERPARAM1?: string;\nPAYMETHOD?: string;\nDISABLEPAYMETHOD?: string;\nPAYMETHODS?: string;\nEMAIL?: string;\nREFERENCENUMBER?: string;\nADDINFO?: string;\nDIGEST?: string;\n};\n\ntype GPWebpayHttpResponse = {\nOPERATION: string;\nORDERNUMBER: number;\nMERORDERNUM?: number;\nMD?: string;\nPRCODE: string;\nSRCODE: string;\nRESULTTEXT?: string;\nUSERPARAM1?: string;\nADDINFO?: string;\nTOKEN?: string;\nEXPIRY?: string;\nACSRES?: string;\nACCODE?: string;\nPANPATTERN?: string;\nDAYTOCAPTURE?: string;\nTOKENREGSTATUS?: string;\nACRC?: string;\nRRN?: string;\nPAR?: string;\nTRACEID?: string;\nDIGEST: string;\nDIGEST1: string;\n};\n\nexport const checkPaymentKeys = () => {\ntry {\n// const publicKey = readFileSync(webpayConfig.publicKeyPath)\nconst privateKey = readFileSync(GP_WEBPAY_PRIVATE_KEY_PATH);\nconst gpPublicKey = readFileSync(GP_WEBPAY_PUBLIC_KEY_PATH);\n\nif (\n// publicKey.length === 0 ||\nprivateKey.length === 0 ||\ngpPublicKey.length === 0\n) {\nthrow new Error('Empty key files');\n}\n\nif (!process.env.GP_WEBPAY_PRIVATE_KEY_PASSWORD) {\nthrow new Error('Empty private key password');\n}\n\nif (!process.env.GP_WEBPAY_MERCHANT_NUMBER) {\nthrow new Error('Empty merchant number');\n}\n} catch (err) {\nconsole.log(err);\nreturn false;\n}\nreturn true;\n};\n\nconst createRequestSignatureString = (\npaymentObject: GPWebpayHttpRequest\n): string => {\n// DO NOT CHANGE ORDER OF PARAMS\nlet data: string;\ndata = `${paymentObject.MERCHANTNUMBER}|${paymentObject.OPERATION}|${paymentObject.ORDERNUMBER}|${paymentObject.AMOUNT}`;\ndata += paymentObject.CURRENCY ? `|${paymentObject.CURRENCY}` : '';\ndata += paymentObject.DEPOSITFLAG ? `|${paymentObject.DEPOSITFLAG}` : '';\ndata += paymentObject.URL ? `|${paymentObject.URL}` : '';\ndata += paymentObject.USERPARAM1 ? `|${paymentObject.USERPARAM1}` : '';\nreturn data;\n};\n\n// In case of DIGEST1 verification use withMerchantNumber = true\nconst createResponseSignatureString = (\nresponseObject: GPWebpayHttpResponse,\nwithMerchantNumber = false\n): string => {\n// DO NOT CHANGE ORDER OF PARAMS\nlet data: string;\ndata = `${responseObject.OPERATION}`;\ndata += `|${responseObject.ORDERNUMBER}`;\ndata += responseObject.MERORDERNUM ? `|${responseObject.MERORDERNUM}` : '';\ndata += responseObject.MD ? `|${responseObject.MD}` : '';\ndata += `|${responseObject.PRCODE}`;\ndata += `|${responseObject.SRCODE}`;\ndata += responseObject.RESULTTEXT ? `|${responseObject.RESULTTEXT}` : '';\ndata += responseObject.USERPARAM1 ? `|${responseObject.USERPARAM1}` : '';\ndata += responseObject.ADDINFO ? `|${responseObject.ADDINFO}` : '';\ndata += responseObject.TOKEN ? `|${responseObject.TOKEN}` : '';\ndata += responseObject.EXPIRY ? `|${responseObject.EXPIRY}` : '';\ndata += responseObject.ACSRES ? `|${responseObject.ACSRES}` : '';\ndata += responseObject.ACCODE ? `|${responseObject.ACCODE}` : '';\ndata += responseObject.PANPATTERN ? `|${responseObject.PANPATTERN}` : '';\ndata += responseObject.DAYTOCAPTURE\n? `|${responseObject.DAYTOCAPTURE}`\n: '';\ndata += responseObject.TOKENREGSTATUS\n? `|${responseObject.TOKENREGSTATUS}`\n: '';\ndata += responseObject.ACRC ? `|${responseObject.ACRC}` : '';\ndata += responseObject.RRN ? `|${responseObject.RRN}` : '';\ndata += responseObject.PAR ? `|${responseObject.PAR}` : '';\ndata += responseObject.TRACEID ? `|${responseObject.TRACEID}` : '';\ndata += withMerchantNumber\n? `|${process.env.GP_WEBPAY_MERCHANT_NUMBER}`\n: '';\nreturn data;\n};\n\nconst signData = async (paymentObject: GPWebpayHttpRequest) => {\nconst privateKeyPassword = process.env.GP_WEBPAY_PRIVATE_KEY_PASSWORD;\nif (!privateKeyPassword)\nthrow new Error('Missing GP_WEBPAY_PRIVATE_KEY_PASSWORD env var');\nconst dataToSign = createRequestSignatureString(paymentObject);\n// const publicKey = await promises.readFile(webpayConfig.publicKeyPath)\nconst privateKey = await promises.readFile(GP_WEBPAY_PRIVATE_KEY_PATH);\nreturn createSignature(dataToSign, privateKey, privateKeyPassword);\n// self-verify signature\n// if (verifySignature(dataToSign, signature, publicKey) === false) {\n// \tthrow new Error('Problem with verifying signature, check payment keys.')\n// }\n};\n\nexport const verifyPaymentSignature = async (\npaymentResponse: GPWebpayHttpResponse\n) => {\nconst data = createResponseSignatureString(paymentResponse);\nconst dataWithMerchantNumber = createResponseSignatureString(\npaymentResponse,\ntrue\n);\nconst publicKey = await promises.readFile(GP_WEBPAY_PUBLIC_KEY_PATH);\nreturn (\nverifySignature(data, paymentResponse.DIGEST, publicKey) &&\nverifySignature(\ndataWithMerchantNumber,\npaymentResponse.DIGEST1,\npublicKey\n)\n);\n};\n\nexport const verifyPaymentResult = (paymentResponse: GPWebpayHttpResponse) =>\n// AK PRCODE && SRCODE === 0 => PLATBA PREBEHLA V PORIADKU\nparseInt(paymentResponse.PRCODE, 10) === 0 &&\nparseInt(paymentResponse.SRCODE, 10) === 0;\n\ntype CreatePaymentParams = {\norderNumber: number;\namountInCents: number;\ntransactionId: string;\n};\n\nexport const createPayment = async ({\norderNumber,\namountInCents,\ntransactionId,\n}: CreatePaymentParams) => {\nconst merchantNumber = process.env.GP_WEBPAY_MERCHANT_NUMBER;\nif (!merchantNumber)\nthrow new Error('Missing GP_WEBPAY_MERCHANT_NUMBER env var');\nconst paymentObject: GPWebpayHttpRequest = {\nMERCHANTNUMBER: merchantNumber,\nOPERATION: PAYMENT_OPERATION.CREATE_ORDER,\nORDERNUMBER: orderNumber, // must be unique\nAMOUNT: amountInCents,\nCURRENCY: Number(process.env.GP_WEBPAY_CURRENCY), // num code based on ISO 4217\n// TODO check below, whether we can't (or whether we should) work with 0 as well\nDEPOSITFLAG: 1, // 1 - Require immediate payment | 0 - Do not require immediate payment\nURL: `${process.env.GP_WEBPAY_CLIENT_APP_URL}/api-mpa/payment-response-redirect/${transactionId}`, // BE URL where will result be sent\n};\n\nconst signedObject: GPWebpayHttpRequest = {\n...paymentObject,\nDIGEST: await signData(paymentObject),\n};\n\nreturn {\nurl: process.env.GP_WEBPAY_HTTP_API_URL,\ndata: signedObject,\ndataToSign: createRequestSignatureString(paymentObject),\nformurlencoded: formurlencoded(signedObject, { ignorenull: true }),\n};\n};\n\nCreate endpoints into app.controller.ts for:3.1) send request into GP webpay3.2) get data from GP and check if payment was successful3.3) redirect from check payment to payment was correct or payment failedExample:\n@Get('payment-gateway-redirect/:orderNumber')\n@Redirect('www.TODO-what-to-put-here.com', 302)\nasync paymentGatewayRedirect(@Param('orderNumber') orderNumber: string) {\n// TODO that orderNumber is an integer might get validated via pipe ?\n// TODO validate that such order was previously created in db\nconst { url, formurlencoded } = await createPayment({\norderNumber: Number.parseInt(orderNumber),\namountInCents: 100,\ntransactionId: '123',\n});\n// const result = await this.appService.ticketPayment(data, { orderNumber });\nreturn { url: `${url}?${formurlencoded}` };\n}\n\n@Get('payment-response-redirect/:id')\n@Redirect('/api-mpa/payment-success', 302)\nasync paymentResponseRedirect(\n@Request() req: IncomingMessage,\n@Param('id') id: string\n) {\nconst result = await this.appTicketService.checkCardPayment(id, req);\nreturn { url: `${process.env.GP_WEBPAY_CLIENT_APP_URL}/api-mpa/${result}` };\n}\n\n@Get('payment-success')\nasync paymentResponseSuccess() {\nconsole.log('success');\n// we need an url that frontend will recognize as successfully accepted payment\n}\n\n@Get('payment-failure')\nasync paymentResponseFailure() {\nconsole.log('failure');\n// we need an url that frontend will recognize as failed payment that warrants restarting the whole payment process\n}\n\n@Get('parksys-failure')\nasync paymentParksysResponseFailure() {\nconsole.log('parksys failure');\n// we need an url that frontend will recognize as failed payment that warrants restarting the whole payment process\n}"}},"/nestjs/Controllers":{"title":"Controllers","data":{"":"More documentation here"}},"/nestjs/Kubernetes":{"title":"Kubernetes","data":{"":"deploy to kubernetes and environments are setup in kustomize/kustomization.ymlExample:\nresources:\n- ../../../../kustomize/bases/database\nnamePrefix: nest-{{app-name}}-\ncommonLabels:\napp: nest-{{app-name}}\n\n# Apply ConfigMap\npatches:\n# this target is about setup deployments, pairing secret values and replace env values with this values\n- target:\nname: app\nkind: Deployment\npatch: |-\n- op: add\npath: /spec/template/spec/containers/0/env\nvalue:\n- name: POSTGRES_PASSWORD\nvalueFrom:\nsecretKeyRef:\nname: nest-{{app-name}}-db\nkey: POSTGRES_PASSWORD\n- name: DATABASE_URL\nvalue: postgresql://nest-{{app-name}}:$(POSTGRES_PASSWORD)@nest-{{app-name}}-database:5432/nest-{{app-name}}?schema=public\n- name: PARKSYS_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: nest-{{app-name}}-posam\nkey: PARKSYS_CLIENT_SECRET\n- op: add\npath: /spec/template/spec/containers/0/envFrom\nvalue:\n- configMapRef:\nname: nest-{{app-name}}\n# this target is about ingress and setup container url by ingres into bratislava.sk\n- target:\nname: ingress\nkind: Ingress\npatch: |-\n- op: replace\npath: /spec/rules/0/host\nvalue: $URL_NEST_{{app-name}}\n- op: replace\npath: /spec/tls/0/hosts/0\nvalue: $URL_NEST_{{app-name}}\n# setup running app on port inside container\n- target:\nname: app\nkind: Service\npatch: |-\n- op: replace\npath: /spec/ports/0/targetPort\nvalue: 3333\n# setup running database inside container\n- target:\nname: database\nkind: StatefulSet\npatch: |-\n- op: add\npath: /spec/template/spec/containers/0/envFrom\nvalue:\n- secretRef:\nname: nest-{{app-name}}-db\n\n# Rename Images\nimages:\n- name: node\nnewName: $REPOSITORY/nest-{{app-name}}\nnewTag: $TAG"}},"/nestjs/Logging":{"title":"Logging","data":{"":"For now use nest default logger, but it will be adjusted in few months","installation#Installation":"nothing","setup#Setup":"create service services/vendors/my-logger.service.ts:\nimport { Injectable, NestMiddleware, Logger } from '@nestjs/common';\n\nimport { Request, Response, NextFunction } from 'express';\n\n@Injectable()\nexport class AppLoggerMiddleware implements NestMiddleware {\nuse(request: Request, response: Response, next: NextFunction): void {\nconst { ip, method, originalUrl } = request;\n\nconst userAgent = request.get('user-agent') || '';\nresponse.on('close', () => {\nconst { statusCode } = response;\nconst contentLength = response.get('content-length');\nlet status = 'HTTP';\nif (statusCode >= 400) {\nstatus = 'ERROR HTTP';\n}\nconst logger = new Logger(status);\n\nlogger.log(\n`${method} ${originalUrl} ${statusCode} ${contentLength} - ${userAgent} ${ip},  request-body: ${JSON.stringify(\nrequest.body\n)}, response-data: ???`\n);\n});\n\nnext();\n}\n}\n\nupdate app.module.ts\nexport class AppModule {}\ninto\nexport class AppModule implements NestModule {\nconfigure(consumer: MiddlewareConsumer): void {\nconsumer.apply(AppLoggerMiddleware).forRoutes('*');\n}\n}"}},"/nestjs/Module":{"title":"Module","data":{"":"Into module you can insert used controllers and services. If you are using more apps, please create one base module.ts and import other modules into this one.NOTE: services, which are used with single controller in a single (sub)module need to be inserted into this submodule, not into the root module.Example of submodule.ts:\n@Module({\nimports: [],\ncontrollers: [MainAppController, AppController],\nproviders: [AppSessionService, AppAddOnsService, PrismaService],\n})\nexport class AppSubModule {}\nExample of module.ts:\n@Module({\nimports: [AppSubModule1, AppSubModule2],\ncontrollers: [AppHealthController],\nproviders: [],\n})\nexport class AppModule {}\nMore documentation here"}},"/nestjs/NestJS":{"title":"Nestjs","data":{"":"What is the nestjs structure, what plugins and decorators do we use and how do we use it.","docs-topics#Docs topics:":"Project Structure\nmodule\ncontrollers\nservices\n\n\nDocumentation and maintenance\nOpenAPI\nLogging\nSentry\nTesting\n\n\nDatabase connectors\nPrisma\n\n\nOther services\nAuthentication\nCard WebPay\nExternal REST API\n\n\nRelease\nKubernetes"}},"/nestjs/OpenAPI":{"title":"Openapi","data":{"":"Some of this is out-of-date / legacy from the projects being part of single monorepo\nWe are generating OpenAPI specification for all of our NestJS projects. To generate this specification we're using @nestjs/swagger package.","decorators-and-introspection#Decorators and introspection":"We're using typescript plugin for introspecting comments that we don't need to write decorators for classes and descriptions for methods. This allows for using same documentation for OpenAPI and jsdoc for IDE like VSCode.This extension needs to be included in project.json:\n{\n...\n\"build\": {\n\"executor\": \"@nrwl/node:build\",\n\"outputs\": [\"{options.outputPath}\"],\n\"options\": {\n...\n\"tsPlugins\": [\n{\n\"name\": \"@nestjs/swagger/plugin\",\n\"options\": {\n\"introspectComments\": true\n}\n}\n]\n}\n}\n}","dtosentities#DTOs/Entities":"Interpolation happens in *.dto.ts and *.entity.ts (basic difference is dto is only for transfers, entity can be used internally, but it depends on internal standards).Every file with this name should contain a default exported Class. The properties of this class will be automatically included in OpenAPI specs even without decorators.To add description and example, we can use jsdoc spec instead of decorators (description can contain markdown - see next chapter).","userdtots#user.dto.ts":"export class User {\n/**\n* Name of user\n* @example John\n*/\nname: string\nemail: string\n}\ngenerates:\nschemas:\nUser:\ntype: object\nproperties:\nname:\ntype: string\ndescription: Name of user\nexample: John\nemail:\ntype: string\nrequired:\n- name\n- email\nand this is it displayed in swagger:","controllers#Controllers":"Controllers also support some level of comment introspection. It unfortunately only works for method description. We can use comment before method decorator to include that description (supports markdown).For responses we need to include description in the decorator. It also supports markdown, but be aware that you don't want any extra whitespace (no indenting).\n\nimport { Controller, Get } from \"@nestjs/common\"\nimport { ApiResponse } from \"@nestjs/swagger\"\nimport { User } from \"./user.dto\"\n\n@Controller()\nexport class AppController {\n/**\n* ## Gets user\n* Some extra stuff\n*/\n@Get()\n@ApiResponse({\nstatus: 200,\ntype: User,\ndescription: `# Testing markdown response description\nReturns user on success\n`,\n})\ngetData(): User {\nreturn new User()\n}\n}\nThis produces following swagger","markdown-documentation#Markdown documentation":"As it was already mentioned, we can use markdown comments in model and method descriptions and markdown strings in response descriptions (and also global description).But with raw-loader and typescript types (files in /config/ need to be imported in project.json and tsconfig)","projectjson#project.json":"...\n\"build\": {\n\"executor\": \"@nrwl/node:build\",\n\"outputs\": [\"{options.outputPath}\"],\n\"options\": {\n...\n\"webpackConfig\": [\n\"config/raw-loader.webpack.js\"\n]\n}\n}","tsconfigappjson#tsconfig.app.json":"{\n...\n\"compilerOptions\": {\n...\n\"types\": [\"node\", \"../../../config/raw-loader\"]\n}\n}\n\nThen we can import markdown files directly using an import statement. This is useful mainly for the main documentation of the API (please have in mind that it has to be a single page). This allows to reuse the documentation that's written in our docs (imported as @bratislava/docs).","testmd#test.md":"## Test API\n\nTesting the markdown import\n\n> With blockquote\n\n```ts\nconst user = new User()\n```\n\n- This\n- _is_\n- **working**\nImporting the file like this:\nimport docs from '@bratislava/docs/apps/test.md';\n\nasync function bootstrap() {\nconst app = await NestFactory.create(AppModule);\nconst port = process.env.PORT || 3333;\n\nconst config = new DocumentBuilder()\n.setTitle('Test')\n.setDescription(docs)\n.setVersion('1.0')\n.build();\n...\nAnd it looks like following in Swagger UI"}},"/nestjs/Prisma":{"title":"Prisma","data":{"":"Some of this is out-of-date / legacy from the projects being part of single monorepo","setup#Setup":"Update prisma/schema.prisma with\ngenerator client {\nprovider = \"prisma-client-js\"\noutput   = \"../../../../node_modules/.prisma/client/nest-{app-name}\"\n}\nand create for prod build prisma/schema.prisma.prod with\ngenerator client {\nprovider = \"prisma-client-js\"\noutput   = \"../node_modules/.prisma/client/nest-{app-name}\"\n}\n\nCreate models in prisma/schema.prisma\nGenerate and migrate prisma\nCreate new service in services/database or in prisma folder prisma.service.ts\nimport { INestApplication, Injectable, OnModuleInit } from '@nestjs/common';\nimport { PrismaClient } from '.prisma/client/app-name';\n\n@Injectable()\nexport class PrismaService extends PrismaClient implements OnModuleInit {\nasync onModuleInit() {\nawait this.$connect();\n}\n\nasync enableShutdownHooks(app: INestApplication) {\nthis.$on('beforeExit', async () => {\nawait app.close();\n});\n}\n}\n\nCreate repository files services/database/repository/{{model-name}}.repository.ts for each entity you want to communicate with database\nUse prisma by its documentation or prisma in Nest.js documentationEXAMPLE:  {{model}}.repository.ts\nexport class {{Model}}Repository {\nconstructor(private prisma = new PrismaService()) {}\n\nasync create{{Model}}(data: Prisma.{{Model}}CreateInput): Promise<{{Model}}> {\nreturn this.prisma.{{model}}.create({\ndata,\n});\n}\n\nasync get{{Model}}(\n{{model}}WhereUniqueInput: Prisma.{{Model}}WhereUniqueInput\n): Promise<{{Model}} | null> {\ntry {\nconst data = await this.prisma.{{model}}.findUnique({\nwhere: {{model}}WhereUniqueInput,\n});\nif (data === null) {\nthrow new HttpException({ message: 'data not found' }, 400);\n}\nreturn data;\n} catch {\nthrow new HttpException({ message: 'data not found' }, 400);\n}\n}\n\nasync get{{Model}}(getData: DtoForGetting): Promise<{{Model}}[]> {\nconst where = {\nvariable1: {\nequals: false,\n},\nvariable2: {\nequals: get{{Model}}.variable2,\n},\n};\nconst data = await this.prisma.{{model}}.findMany({\n// skip: data[skip],\n// take: data[take],\nwhere: where,\norderBy: [\n{\nupdated_at: 'desc',\n},\n],\n});\n\nreturn data;\n}\n\nasync update{{Model}}(params: {\nwhere: Prisma.{{Model}}WhereUniqueInput;\ndata: Prisma.{{Model}}UpdateInput;\n}): Promise<{{Model}}> {\nconst { where, data } = params;\nreturn this.prisma.{{model}}.update({\ndata,\nwhere,\n});\n}\n}"}},"/nestjs/ProjectStructure":{"title":"Projectstructure","data":{"":"This part will show you, how to create basic structure of your NEST application. How to generate project and how to prepare folders and document structure. If you are familiar with nest after, this you can start programming. If not, you can check nest documentation https://docs.nestjs.com, or check other parts of this docs.","file-structure#File structure":"Consider if you will need two or more separate apps in backend or you need only one.","main-basic-structure-is#Main basic structure is:":"├── src\n│   ├── app\n│   │   ├── app.controller.ts\n│   │   ├── app.service.ts\n│   │   ├── app.dtos.ts\n│   │   ├── app.models.ts\n│   │   ├── app.modul.ts\n│   ├── main.ts","complex-structure-with-prisma-is#Complex structure with prisma is:":"├── prisma\n│   ├── migrations\n│   ├── schema.prisma\n│   ├── schema.prisma.prod\n│   ├── seed.ts\n├── src\n│   ├── app1\n│   │   ├── controllers\n│   │   │   ├── app.controller1.ts\n│   │   │   ├── app.controller2.ts\n│   │   ├── services\n│   │   │   ├── app.service1.ts\n│   │   │   ├── app.service2.ts\n│   │   │   ├── database\n│   │   │   │   ├── repository\n│   │   │   │   │   ├── app.repository1.ts\n│   │   │   │   │   ├── app.repository2.ts\n│   │   │   │   ├── database.service.ts\n│   │   │   ├── vendors\n│   │   │   │   ├── vendor1.service.ts\n│   │   │   │   ├── vendor2.service.ts\n│   │   ├── dto\n│   │   │   ├── app.dto1.ts\n│   │   │   ├── app.dto2.ts\n│   │   ├── public\n│   │   ├── app1.module.ts\n│   ├── app2\n│   │   ├── ... (same as app1)\n│   │   ├── app2.module.ts\n│   ├── module.ts\n│   ├── main.ts\nNOTE:\nPlease use logical names by type of project instead of app if you have more than one app or more then 1 service | controller | repository ...\nif you have one app please use only one module.ts, else you can import all modules into main module which is used in main.ts"}},"/nestjs/RestApi":{"title":"Restapi","data":{"":"please use httpModule under Axios","install#Install":"npm i @nestjs/axios","setup#Setup":"Insert into app.module.ts into imports:\nimport { HttpModule } from '@nestjs/axios';\n\n@Module({\nimports: [HttpModule],\ncontrollers: [AppController],\nproviders: [AppService],\n})\nexport class AppModule {}\n\nUse documentation for axios"}},"/nestjs/Sentry":{"title":"Sentry","data":{"":"","installation#Installation":"@ntegral/nestjs-sentry","setup#Setup":"Add into app.module.ts into imports:\n@Module({\nimports: [\nSentryModule.forRoot({\ndsn: 'https://{{id}}@o{{id}}.ingest.sentry.io/{{id}}',\ndebug: false,\nenvironment: 'production',\nrelease: APP_VERSION, // must create a release in sentry.io dashboard\ntracesSampleRate: 1.0,\n}),\n],\ncontrollers: [AppController],\nproviders: [AppService],\n})\n\nAdd into your controller file new sentry decorator:\n@Controller('api')\n@UseInterceptors(new SentryInterceptor())\nexport class AppController {\n@Get('healthcheck')\nhealthcheck(): string {\nreturn 'OK';\n}\n}"}},"/nestjs/Services":{"title":"Services","data":{"":"More documentation here"}},"/nestjs/Testing":{"title":"Testing","data":{"":"TBD"}},"/other/developer-accesses":{"title":"Developer Accesses","data":{"":"List of accesses for internal developers & contractors. Does not go into documents signed or other HR stuff 🙂","general#General":"Slack\nJira / Confluence\n@bratislava email and access to Bratislava VPN - usually goes hand in hand and is needed only for internal users\nVema (BA HR system) access - accessible only from Magistrate network, internals only\nadd to OIADS_EMPLOYEES AD group - again for internals, this gives access to Harbor, Grafana, Minio CDN and apps deployed on DEV cluster. Guests / external users can be arranged, either via different group or assigned to the apps directly.\nGithub\nAzure devops (mainly for database backups)\nMiro - mainly for weekly retrospectives, internals only","optional#Optional":"Not needed for everyone or needed for specific roles - usually not needed during onboarding, listed so that we know about the services used\nFigma - this is often per-project\nGoogle Cloud Console - google specific services (i.e. places api, recaptcha)\nMailgun\nMailchimp\nSentry - error monitoring and alerting\nExpo.io - if you are working on mobile apps\nInovacie gmail account\nBratislava's google analytics account\nPlausible.io analytics\nHotjar analytics\nMapbox\nWebsupport (one for Bratislava, multiple others for other clients/projects)\nAppleID (mostly for mobile dev)"}},"/other/meetings-syncs-rituals":{"title":"Meetings Syncs Rituals","data":{"":"These are for developers across all projects in the Inovations team. Each project may have it's own planning/standup/retrospective meets & calls, we're not covering those here.","regular-meetings--calls#Regular meetings / calls":"On Monday at 9:00 there is a department-wide weekly standup, where we all (including the non-devs) share our agenda for the upcoming week.Every weekday at 9:00 (except for Monday) there is a short developer-only standup. This is mainly for sharing any potential blockers and getting in touch with people on the team you may need for the day.\nIf you can't attend the morning standup please use #stand-up slack channel for a written version instead. The non-dev part of the team uses this each day, except for Monday.\nEvery Friday at 11:00 we have a department wide retrospective session. This works mainly to keep in touch across all the different projects we work on, to celebrate goals we've reached and share any learnings with the rest of the team.","vacations--sick-days--home-office#Vacations / sick-days / home-office":"Communicate all of these through slack with relevant people as well as through vema."}},"/recipes/connecting-to-kubernetes":{"title":"Connecting to Kubernetes","data":{"":"This section briefly shows how to do some basic connecting to Service and PODs in kubernetes cluster through kubectl or kubernetes Lens.\nAs stated you will obviously need access to our cluster. To login to specific cluster you can follow our Login guide.","how-to-connect-to-database#How to connect to database":"","lens#Lens":"Open Lens and on the left hand side panel navigate to Network -> Services.\nFind the Service that you want to connect to, usually something like anything-database or anything-database-service\nClick on right service and scroll down to find Connection section and under there should be a button Forward. Click it and choose your local free port\nNow your database should be accessible through localhost\n\nNote: it is password free, but if you want to find password to db, go to secrets and find db secret for this db","kubectl#kubectl":"You can also port forward directly through kubectl command by running\nkubectl port-forward service/anything-database-service 5432:5432\nFor more information checkout official port-forward documentation.","how-to-see-logs#How to see logs":"","lens-1#Lens":"You can follow this official video guide on extracting Logs https://www.youtube.com/watch?v=QIHqOd05TBA.","kubectl-1#kubectl":"You can get pod logs in you terminal with kubectl by running\nkubectl get logs my-pod\nFor more information, check out interacting with running pods.","how-to-connect-to-apppod-terminal#How to connect to app/pod terminal":"","lens-2#Lens":"Open Lens and on the left hand side panel navigate to Workloads -> Pods\nFind the POD in question and click on it\nClick in right top corner find Pod Shell button (next to Pod Logs)","kubectl-2#kubectl":"If you prefer to attach to POD within your terminal, you can do so with kubectl\nkubectl exec -i -t --namespace <namespace> <pod_name> --container <container> -- sh -c \"clear; (bash || ash || sh)\"\nFor example, to attach to pod with name \"next-city-gallery-app-debug\" in a namespace \"bratislava-monorepo\" and container \"app\", run the following\nkubectl exec -i -t --namespace bratislava-monorepo next-city-gallery-app-debug --container app -- sh -c \"clear; (bash || ash || sh)\"","tips--tricks#Tips & Tricks":"If you are running reasonable *nix terminal you can automate most of these task from your terminal, here are just some examples","attach-to-pod-within-your-terminal-without-knowing-full-name#Attach to POD within your terminal without knowing full name":"You can create a simple script, for example k8att.sh, that will search and attach to the right POD and it's container within your terminal. As this will search through available PODs you don't need to know full name to attach to as sometimes the PODs name can be long and in different namespaces.\n#!/bin/bash\n\nif [[ -z \"$1\" ]]\nthen\necho \"ERROR: You have to give us name of the pod to attach to\"\nexit 1\nfi\n\nPOD_RESP=$(kubectl get pods --all-namespaces | grep -E \"$1\")\nPOD_NAME=$(echo \"$POD_RESP\" | awk '{print $2}')\nPOD_NAMESPACE=$(echo \"$POD_RESP\" | awk '{print $1}')\n\nPOD_CONTAINERS=$(kubectl get pod --namespace \"$POD_NAMESPACE\" \"$POD_NAME\" -o jsonpath='{.spec.containers[*].name}')\nIFS=' ' read -ra CONTAINERS <<< \"$POD_CONTAINERS\"\n\nif [ \"${#CONTAINERS[@]}\" -gt 1 ]; then\nfor i in \"${!CONTAINERS[@]}\"; do\nprintf \"%s\\t%s\\n\" \"$i\" \"${CONTAINERS[$i]}\"\ndone\nread -p \"Choose container index (expecting int): \" CONTAINER_IX\nelse\nCONTAINER_IX=0\nfi\n\nCONTAINER=${CONTAINERS[$CONTAINER_IX]}\necho -e \"Attaching to pod \\033[0;32m'$POD_NAME'\\033[0m, container \\033[0;32m'$CONTAINER'\\033[0m in \\033[0;34m'$POD_NAMESPACE'\\033[0m namespace \"\nkubectl exec -i -t --namespace \"$POD_NAMESPACE\" \"$POD_NAME\" --container \"$CONTAINER\" -- sh -c \"clear; (bash || ash || sh)\"\nThen you just chmod +x k8att.sh and you can use it to attach. Sticking with our previous attach example, now to attach to a pod \"next-city-gallery-app-debug\" you can just do\n./k8att gallery-app-debug\nIf it has more containers to choose from, the script will prompt you to pick ID of the one that you want to attach to. Script use grep extended regex syntax, therefore you can also pass valid regex to it, e.g., gallery-[a-z]+-debug.A very similar use are pod logs for which you can basically use the same script just adjust the last line to kubectl get logs.You can add these scripts to your aliases in ~/.bashrc or ~/.bash_aliases\nalias k8att='~/k8att.sh'\nalias k8logs='~/k8logs.sh'\nand just use it as any other command in your workflow."}},"/recipes/debug-cluster-connectivity":{"title":"Debug Cluster Connectivity","data":{"":"To debug running pods in kubernetes you can follow official guide - https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/.\nFollowing is just a small example in our local context. Let's assume you want to check if you can ping a service from the next-city-library pod.","connect-to-proper-kubernetes-cluster#Connect to proper Kubernetes cluster":"In our case, we will connect to tkg-innov-dev. You can follow our Login guide.\nTo verify that you are correctly connected you can run\nkubectl config current-context\nThe output should match tkg-innov-dev.","check-if-your-service-is-running-in-a-pod#Check if your service is running in a pod.":"Run\nkubectl get pods --all-namespaces\nand find your desired pod and copy its name like: next-city-library-app-b868fcd5c-lj6tb.\nIf you are running a reasonable *nix shell, you can do\nkubectl get pods --all-namespaces | grep \"next-city-library\"\nwhich should help with finding your POD.","create-a-new-debug-pod-with-ubuntu#Create a new debug pod with ubuntu":"Now we will create a new debug pod with ubuntu, and we will attach the container to the pod so we can easily debug it.\nRun following command to create a new debug POD, which will be a exact copy of your original container (you can read more information in the official docs)\nkubectl debug next-city-library-app-b868fcd5c-lj6to -it --namespace=bratislava-monorepo --image=ubuntu --share-processes --copy-to=next-city-library-app-debug\nDon't forget to update the proper pod and --copy-to to the name you want. Under --copy-to, you will find the running pod. This command will also attach you to newly created POD.","install-needed-debugging-utilities-for-ubuntu#Install needed debugging utilities for ubuntu":"Before any installations run, you might need to run\nsudo apt update --yes\nFor debugging, you will probably need some networking tools. You can install it with these commands:\nping: apt-get install iputils-ping\nnslookup: apt-get install dnsutils\nwget: apt install wget\ncurl: apt install curl","clean-up#Clean Up":"After you are done with debugging, please clean up after yourself and delete the pods that you have created and no longer need. For example you can delete our debug pod with following command\nkubectl delete pod next-city-library-app-debug --namespace=bratislava-monorepo"}},"/recipes/docker-setup":{"title":"Docker Setup","data":{"":"If you are on Linux or macOS, we recommend installing Docker desktopIf you are running from Windows WSL (setup here), you might be better off using podman instead. In this case, you'll also need to install & setup podman-compose. Use podman and podman-compose as substitute everytime you see docker or docker compose."}},"/recipes/editor-setup":{"title":"Editor Setup","data":{"":"We recommend using VS Code with the following extensions installed and formatting your code on save enabled (or, at minimum, formatting before commit):\nEslint\nPrettier\n\nTo setup vscode, open settings.json and insert following lines.The organizeImports will remove all unused imports, formatDocument will run prettier an fixAll.eslint will run eslint fix. The order of these commands is important.The files.insertFinalNewline setting will add a new line at the end of the document.\n\"editor.codeActionsOnSave\": {\n\"source.organizeImports\": true,\n\"source.formatDocument\": true,\n\"source.fixAll.eslint\": true\n},\n\"files.insertFinalNewline\": true,\n\n// this is optional but recommended\n\"typescript.preferences.importModuleSpecifier\": \"project-relative\",\n\"files.autoSave\": \"onFocusChange\",\n\nIf you prefer a different editor, it's completely fine. Still, you should find the counterparts of the extensions and setup mentioned above that help you with formatting and a suitable typescript integration (which is a part of default VS Code installation).","optional-extensions#Optional extensions":"These are not needed but either nice to have or project-specific. Again listing VS Code extensions, users of different editors need to find their counterparts:\nTailwind CSS Intellisense - for frontend devs\nPath Intellisense\nMarkdown Preview Mermaid Support - for easier reading of .md files\nDotENV - for .env files syntax highlighting\nAuto Rename Tag - for easier React and html tags renaming\nAuto Close Tag - for automatically inserting closing tag\nCode Spell Checker - for spell checking your code, markdown, etc."}},"/recipes/git-workflow":{"title":"Git Workflow","data":{"":"To contribute to our project you will need a account on GitHub and git. Some of the systems (some of the *nix and Mac) already come with git pre-installed, which you can check by running\ngit --version\nIf you don't have it installed you can follow these guides for Linux/Unix or Mac. For windows, you can use gitforwindows.","branches-and-environments#Branches and environments":"We use a single trunk (master) from which feature branches are created. Master is what is regularly deployed to staging environment. Production builds are created for tagged commits, usually from master, but if a quick production fix is needed, the process is to branch-off at the last tagged commit and create new production tags on this new branch.There is no develop or production branch, and there should rarely be need for a long-running branch other than master.","commit-message-branch-and-pr-name-formats#Commit message, branch and PR name formats":"Preferred way of branch naming is {issue-id}/{2-3 word summary}, for example MKB-460/form-captcha.Titles of pull requests should begin with issue id and one sentence description, for example \"MKB-460: Add Google Recapcha to page forms\".For commit messages and pull request description please follow the conventions the way they are written here https://namingconvention.org/git/. If multiple options are provided (i.e. for commit message formatting), then all of them are ok.","labels--assignees#Labels & Assignees":"We are using labels and assignees to navigate easily through the open pull requests. Labels should be self explanatory. Assignee should always be the person whose action is required to move the PR forward. That is, if the PR is waiting for review by someone, the reviewer should be an assignee. If a code change is expected by a person, or a question needs to be answered, assign the PR to the person making the change or capable of answering. This way you can easily see which PRs are waiting for your actions, request assistance, or know who should you bother if your PR is stuck 🙂.The usual PR workflow involves assigning the PR between a selected reviewer and the author, while also swapping the needs work and needs review labels depending on the current state. Once the reviewer is satisfied, they'll assign a fix & ship label, leaving it up to the author to merge at his or hers convenience, optionally fixing some minor issues before doing so (without the need for a further review).","resolving-conversations#Resolving conversations":"It is helpful to resolve (github) conversations you have started if you feel the topic has been answered. This goes particularly for reviewers doing multiple passes on a single PR - try to resolve what you can before adding more comments on a subsequent pass.","merging--rebasing#Merging & Rebasing":"Squash & merge into master. Liberal use of rebase for cleaning up your own feature branches (mainly if you tend to create and push work-in-progress commits) is encouraged. If your local commits are way out of control, you might be required to do so before your PR is accepted (but this happens rarely).Still, be careful if someone else branches off one of your feature branches - to make his life easier, you should avoid rebasing, if possible, past the point they've branched at - at least until your work is ready to become a part of master."}},"/recipes/kubernetes-lens-setup":{"title":"Kubernetes Lens Setup","data":{"":"You can only do this with sufficient access rights - ask Innovations team or Bratislava IT department if connecting to k8s infrastructure is relevant to you.","install-requirements#Install Requirements":"Install kubectl based brew install kubectl or based on you platform you can follow https://kubernetes.io/docs/tasks/tools/\nDownload and install vsphere tanzu plugin here, you need to be connected on cable or through VPN to magistrate network\nWindows - copy kubectl and kubectl-vsphere to c:/windows/system32 (needed admin rights) otherwise kubectl vsphere login commands will not work\n\n\nInstall kubeseal brew install kubeseal (apt install kubeseal), a tool for generating secrets\nDownload and install Kubernetes Lens, a GUI tool for easier management of cluster deployments\nIf you prefer command line environments there is also a tool called k9s, which can serve as alternative to Lens.","login#Login":"be connected on cable or through VPN to magistrate, where username is your email.\nrun kubectl login commands\nmain login\nkubectl vsphere login --server=10.10.10.1 --insecure-skip-tls-verify -u {{username}}\nto login through Windows you need to set password export KUBECTL_VSPHERE_PASSWORD={{password}}\n\n\nlogins to clusters\nkubectl vsphere login --server=10.10.10.1 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name=tkg-innov-dev -u {{username}}\nkubectl vsphere login --server=10.10.10.1 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name=tkg-innov-staging -u {{username}}\nkubectl vsphere login --server=10.10.10.1 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name=tkg-innov-prod -u {{username}}\nkubectl vsphere login --server=10.10.10.1 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name=tkg-master -u {{username}}\n(optional, you don't need this, its only master cluster)\n\n\n\n\nopen Lens and you should see clusters","tips--tricks#Tips & Tricks":"If you are running reasonable command line environment you can create aliases for these logins.For *nix based command line environment you can add these to your ~/.bash_aliases or ~/.bashrc. For example:\n# k8 login aliases\nalias k8dev='kubectl vsphere login --server=10.10.10.1 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name=tkg-innov-dev -u {{username}} && kubectl config use-context tkg-innov-dev'\n\nalias k8stage='kubectl vsphere login --server=10.10.10.1 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name=tkg-innov-staging -u {{username}} && kubectl config use-context tkg-innov-staging'\nalias k8staging='k8stage'\n\nalias k8prod='kubectl vsphere login --server=10.10.10.1 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name=tkg-innov-prod -u {{username}} && kubectl config use-context tkg-innov-prod'\nThese will also set a correct config context to use for our clusters. To apply changes re-log to your console or called (any relevant equivalent to) source ~/.bashrc. After this, you can log in just by running k8dev. Note, if you don't have $KUBECTL_VSPHERE_PASSWORD environment variable set it will ask you for your NT password."}},"/recipes/postgres-setup":{"title":"Postgres Setup","data":{"":"We recommend running your postgres database in a docker. For docker setup recommendations, look here.","installing-psql#Installing psql":"It's recommended that you have a cli interface to interact with Postgres instances - you can check here to see how to instal psql.","installing-pgadmin#Installing pgAdmin":"If you prefer a GUI to navigate the database, we recommend pgAdmin","setting-up-local-database-with-docker-compose#Setting up local database with docker compose":"Using docker and docker-compose is an easy way to setup . Often you'll have4 a docker-compose.yml file next to a project for this exact purpose. If you need to quickly spin-up a new database, here's a template for it - put it in a docker-compose.yml file somewhere on your computer and run docker compose up in the same directory.\nversion: '3.8'\nservices:\npostgres:\nimage: postgres:latest\nenvironment:\nPOSTGRES_USER: strapi\nPOSTGRES_PASSWORD: password\nPOSTGRES_DB: strapi\nPGPORT: 5432\nexpose:\n- '5432'\nports:\n- '127.0.0.1:5432:5432'\n\nvolumes:\npostgres:\nFeel free to change the user/pw/db_name to whatever suits you."}},"/recipes/production-deployment-checklist":{"title":"Production Deployment Checklist","data":{"":"","what-to-not-forget-when-moving-from-staging-to-live-production#What to not forget when moving from staging to live production.":"","1-set-up-secrets-on-production#1. Set up secrets on production":"Don't forget to check folder /kubernetes/base/secrets and synchronize all needed secrets to the prod cluster.\nHandy commands for secrets:We are saving secrets by kubeseal. You need to run this command that creates the file database.secret.yml, where all our values are encrypted and safe to add to the repository.\nkubeseal --controller-name=sealed-secrets --scope=namespace-wide --namespace=standalone --format=yaml < database.yml > database.secret.yml\nIf you want to propagate a sealed secret to Kubernetes, you can run this command:\nkubectl create -f database.secret.yml","2-deploy-the-app-to-production#2. Deploy the app to production.":"If the project uses pipelines, you can quickly call bratiska-cli with the command bratiska-cli tag prod from the app folder. If you are not using pipelines, you need to:\ngit checkout master\ngit pull\nmove to the folder where is package.json located\nbratiska-cli deploy --production","3-synchronize-databases-from-staging#3. Synchronize databases from staging":"If the deployment has the database, don't forget to move the newest version to production. You can grab the db from azure backup pipelines or download it from the staging cluster directly. If you have a db file, you can run the following:\nHave a look on this guide: https://bratislava.github.io/docs/strapi/sync-strapi-db-to-different-env","4-add-external-hostname-in-ingress#4. Add external hostname in ingress.":"If the web is going to be live in a different address, you must first add bratislavainovuje.sk as your hostname to check if all network features are working correctly.\nLike here:\nspec:\ntls:\n- hosts:\n- marianum-next.bratislava.sk\n- www.marianum-next.bratislava.sk\n- www.bratislavainovuje.sk\n- bratislavainovuje.sk\nsecretName: marianum-next-tls\nrules:\n- host: marianum-next.bratislava.sk\nhttp:\npaths:\n- path: /\npathType: ImplementationSpecific\nbackend:\nservice:\nname: marianum-next-app\nport:\nnumber: 80\n- host: www.marianum-next.bratislava.sk\nhttp:\npaths:\n- path: /\npathType: ImplementationSpecific\nbackend:\nservice:\nname: marianum-next-app\nport:\nnumber: 80\n- host: bratislavainovuje.sk\nhttp:\npaths:\n- path: /\npathType: ImplementationSpecific\nbackend:\nservice:\nname: marianum-next-app\nport:\nnumber: 80\n- host: www.bratislavainovuje.sk\nhttp:\npaths:\n- path: /\npathType: ImplementationSpecific\nbackend:\nservice:\nname: marianum-next-app\nport:\nnumber: 80\nAs you can see marianum-next.bratislava.sk is default app name and bratislavainovuje.sk is new host name.Also, don't forget to update config.json in kubernetes/envs/Prod where the host is specified:\n{\n\"host\": \"bratislavainovuje.sk\"\n}","5-check-if-everything-is-fine#5. Check if everything is fine":"Now you need to check if all website features are working fine and if there are no bugs. It is worth trying external services used on the page if they work away from the bratislava.sk domain.","6-add-a-new-domain-to-the-internal-dns#6. Add a new domain to the internal DNS":"Your new domain needs to be manually added to the internal network DNS. Please get in touch with filip.krajcik@bratislava.sk on this issue. Otherwise, certificate creation will not work!","7-prepare-for-the-final-hostname-move#7. Prepare for the final hostname move":"The new domain must check its current DNS records to avoid misconfiguration. So use this website for nslookup:\nhttps://www.nslookup.io/domains/bratislavainovuje.sk/dns-records/ where you add your desired domain.\nIf the results contain AAAA records, it is necessary to remove them immediately because this can cause an issue. Our kubernetes infrastructure is IPV4 only. So after removal in 15 mins, AAAA records should be empty.\nRecheck if everything is done with the website: https://www.nslookup.io.\nAlso, it is good to check if everything is prepared for let's encrypt where you add your new domain: https://letsdebug.net/","8-add-new-hostname-to-ingress-and-configjson#8. Add new hostname to ingress and config.json":"The time has come, now add to ingress your new hostname, same as you added bratislavainovuje.sk. Also, you need to change a value in config.json to your new hostname:\n{\n\"host\": \"newhostname.sk\"\n}\nLet things propagate, and we are now ready for the moving procedure.","9-set-domain-ttl-needs-to-be-done-few-days-before-the-move#9. Set domain TTL (needs to be done few days before the move)":"Few days before the move, you need to set the TTL of the domain to 5 minute. This will speed up the propagation of the new DNS records. You can do this in the domain provider.\nHour before the move, you need to set the TTL to 1 minute.","10-change-dns-records-to-kubernetes-records#10. Change DNS records to kubernetes records":"Set A records in DNS to kubernetes public IP: 90.176.20.247\nDelete any remaining AAAA records, as this can cause certificate issues.","11-check-the-status#11. Check the status":"After 15 mins, all changes should be propagated, and the new Certificate with a domain should be properly running in our production kubernetes. If something is failing, check the Troubleshooting section.","webpage-is-not-in-kubernetes#Webpage is not in kubernetes":"The webpage link still shows the old version on the old server. Check if DNS was changed correctly and if there are no remaining AAAA records that can point out to the different server.","certificate-is-invalid#Certificate is invalid":"If you receive an error on acme request 404 or 503, there is a chance that some DNS records are not properly routed.\nCheck https://letsdebug.net/ and type the domain there and see if there are some errors.\nCheck if the Certificate is generated for the www domain like www.bratislavainovuje.sk and see if there are the same errors or Certificate is generated without any problem.","other-problem-with-certificate#Other problem with certificate":"If you have any other problem with certificate, please contact Richard Dvorsky, he can help you with this issue."}},"/recipes/sentry-setup":{"title":"Sentry Setup","data":{"":"We're using Sentry to collect FE errors, as well as for FE/BE alerting.","authorization#Authorization":"sentry-cli sends the sourcemaps to sentry.io during the build step. This means it needs SENTRY_AUTH_TOKEN available during this set - this token is set as Sentry.AuthToken pipeline secret variable (and can be read through the inovacie.bratislava Sentry account). To access (your account's) token go here. You can also use Sentry company-wide \"Internal Integration\" token, this is preferred for pipelines or otherwise shared build envs - you can find such token within the Sentry dashboard as well.Local builds on sentry-enabled projects will fail without SENTRY_AUTH_TOKEN set in your environment - when building locally, you need to set this env var.","releaseversion#Release/Version":"We set the SENTRY_RELEASE & NEXT_PUBLIC_SENTRY_RELEASE - you can override this behaviour per project.","local-build#Local build":"","nextjs-setup#Next.js setup":"See sentry.client/server.config.ts & next.config.js files in tax-landing project (TODO soon to be opened). Notice also all the SENTRY or NEXT_PUBLIC_SENTRY (see the next section why you might need both).withSentry needs to be the last transformation on next.config but it does not play nicely with typescript types potentially exported from previous transformations - you'll likely need to force typecast if importing config into a .ts file.","sentry-environment-variables-on-nexts-fe--be#Sentry environment variables on next's FE & BE":"While using SENTRY_RELEASE works for different technologies or even Next server environment, for Sentry library to be able to read the var successfully on frontend it needs to be prefixed as NEXT_PUBLIC_.Therefore, we also provide NEXT_PUBLIC_SENTRY_RELEASE to pipelines. For the similar reasons we have just NEXT_PUBLIC_SENTRY_ENVIRONMENT set in our Next projects to indicate environment - this can be read by both BE & FE Sentry clients."}},"/recipes/wsl":{"title":"Wsl","data":{"":"Here we briefly show to setup a WSL v2 and our bratislava tools within.","install-wsl#Install WSL":"For this you will need administrator access to your computer. There are plenty of guides out there to install wsl. You can most likely follow any of them\nhttps://docs.microsoft.com/en-us/windows/wsl/install\nhttps://pureinfotech.com/install-windows-subsystem-linux-2-windows-10/\nhttps://www.ssl.com/how-to/enable-linux-subsystem-install-ubuntu-windows-10/\n\nBut, with that said, here is our take on it\nFirst you need to enable linux subsystem in your Windows settings.\nHit windows key and search for \"Turn Windows features on or off\", or in Slovak localization it is \"Zapnúť alebo vypnúť súčasti systému Windows\"\nIn the list search for \"Windows Subsystem for Linux\" or again in Slovak localization \"Podsystém Windowsu pre Linux\"\n\n\nOnce enabled, navigate to Microsoft Store and search for Ubuntu (or preferred Linux distro) and hit \"GET\" button. Note: we suggest you install Ubuntu as there might be fewer issues. At the time of this writing the latest version is \"Ubuntu 22.04.1 LTS\"\nNow you should have your Linux distro installed. Hit windows key and open it. This should install/unpack everything necessary it will also prompt you for your Linux username and password (this will be the password that you will use, for example, when you use sudo)\nLastly, upgrade to WSL v2.\nOpen PowerShell with administrator access\nRun wsl -l -v command to see the installed distribution and their wsl versions\nTo change versions, use the command: wsl --set-version <distro name> 2. In our case it is wsl --set-version Ubuntu-22.04 2\n(Optional) You can also set a default wsl version for any future installation by running wsl --set-default-version <Version#>\n\n\nAfter this you can reopen your terminal and you should be running your WSL v2 Linux distribution\n(Optional but probably desired) As a first thing you should probably upgrade and update your system with the latest packages. You can do so by running\nsudo apt upgrade\nsudo apt update","install-podman#Install Podman":"A podman is open source container system like Docker. You can install the latest version by running\necho 'deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/unstable/xUbuntu_22.04/ /' | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:unstable.list\ncurl -fsSL https://download.opensuse.org/repositories/devel:kubic:libcontainers:unstable/xUbuntu_22.04/Release.key | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/devel_kubic_libcontainers_unstable.gpg > /dev/null\nsudo apt update\nsudo apt install -y podman\nNote: in the current Ubuntu upstream repository podman is stuck in version 3.x. This should use \"unstable\" repository, which should have a latest stable podman version (at time of this writing 4.2.x)For most usecases this is full fledge container system and can replace docker. You can even alias it (in your ~/.bashrc or ~/.bash_aliases)\nalias docker=podman\nor create symlink to it. Podman should be installed in /usr/bin, but you can check by running which podman\ncd /usr/bin\nln -s podman docker","podman-compose#podman-compose":"There is also a replacement for docker-compose called podman-compose, but it is shipped as a different (python) package. You can install it by running\npip3 install podman-compose\nAfter that most likely you will also need to run, due to this issue https://github.com/microsoft/WSL/issues/7948\nupdate-alternatives --set iptables /usr/sbin/iptables-legacy\nupdate-alternatives --set ip6tables /usr/sbin/ip6tables-legacy","install-kubectl-and-other-plugins#Install kubectl and other plugins":"","kubectl#kubectl":"To install kubectl tool follow the official guide https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/Verify you installation by running\nkubectl --version","vsphere-plugin#vsphere plugin":"You will also need to download bratislava vsphere plugin, here: https://k8s.tanzu.bratislava.sk/. Note, that you have to be connected to magistrate network.You can download it and install it by running\nwget -qO- https://k8s.tanzu.bratislava.sk/wcp/plugin/linux-amd64/vsphere-plugin.zip | tar xzv\nsudo install -o root -g root -m 0755 kubectl-vsphere /usr/local/bin/kubectl-vsphere\nAfter this you should be able to log into our kubernetes cluster. You can test and login in by following our Login guide.","kubeseal#kubeseal":"And lastly you will also need a client side utility: kubeseal, which you can download here https://github.com/bitnami-labs/sealed-secrets/releases and is used to encrypt sensitive data.You can also use following lines to download and install\nwget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.18.1/kubeseal-0.18.1-linux-$(dpkg --print-architecture).tar.gz -qO- | tar xvz\nsudo install -o root -g root -m 0755 kubeseal /usr/local/bin/kubeseal\nYou can read more about creating secrets here.","install-bratiska-cli#Install bratiska-cli":"If you plan on deploying to our kubernetes cluster, most likely you will need bratiska-cli npm package.\nBut first, you need to install and gain accesses to all of it's prerequisites.","install-npm#Install npm":"First we need to install npm. You can use following script\ncurl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.0/install.sh | bash\ncat <<EOT >> ~/.bashrc\nexport NVM_DIR=\"${HOME}/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"  # This loads nvm\n[ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\"  # This loads nvm bash_completion\nEOT\nsource ~/.bashrc\nnvm install node","install-yarn#Install yarn":"To Install yarn, you can use your preferred version. Here, we will follow official documentation\nnpm install --global yarn\nAdd yarn to your path\necho \"export PATH=\\\"$PATH:${HOME}/.yarn/bin\\\"\" >> ~/.bashrc\nsource ~/.bashrc","install-bratiska-cli-1#Install bratiska-cli":"After successful installation of yarn (to verify you can run yarn --version) follow\nbratiska-cli installation instruction. Please, verify one more time that you have met all of the required prerequisites.Now you should have everything needed to deploy your application through bratiska-cli.\nTo login to our cluster you can follow our Login guide."}},"/recipes/env-vars-and-secrets":{"title":"Env Vars and Secrets","data":{"":"If you want an easy-to-follow guide on where to place your env config based on your project, jump directly to Env vars cookbook - if you're new to this topic it's recommended to read and understand at least the section on Run time vs Build time env vars beforehand\nThere are 4 main sources of configuration for our apps, each group being stored slightly differently:\nlocal development environment variables, these will not be used in any of the deployments\nbuild time environment variables, which need to be available on local machine / in pipelines at build time\nrun time environment variables, accessible by the running (server) instance in Kubernetes\nsecrets stored in Kubernetes using Sealed secrets, accessible at run time\n\nThere are also build-time secrets - but the need for these is rare and we won't go into them.","run-time-vs-build-time#Run time vs Build time":"Our applications run in k8s cluster but are typically built on a local machine or in pipelines. Therefore we distinguish between groups of environment variables based on when they are available to the application.\nif it's accessible on frontend it's typically a build time variable (i.e. next.js preps the frontend bundle during build-time, despite it being also the one serving it run-time)\nif it configures the behaviour of the build itself it's a build time variable - these are usually set-up by the environment itself, and super rare. One example of build-time variable like this is the SENTRY_AUTH_TOKEN which is needed to upload source_maps - but this is usually configured either by using .sentryrc file or is taken from pipelines global config. Other typical one is process.env.CI, which is set by Github pipelines automatically\neverything else is a run time variable - these can be accessed by server at runtime and typically includes secrets which shouldn't leak to end users (or be placed under source control)","local-development-environment-variables#Local development environment variables":"","nextjs#Next.js":"Next.js loads .env.development automatically - values in here can be overwritten using .env.local or .env.development.local. More info in Next.js environment variable order guide","strapi#Strapi":"Our Strapi setup (usually) loads data from .env.local file in development - this file is gitignored and should be created from .env.example during dev setup.","nestjs#Nest.js":"Our nest.js tempalte configuration loads data from .env file in development - this file is gitignored and should be created from .env.example during dev setup.","build-time-environment-variables#Build time environment variables":"","nextjs-1#Next.js":"Use .env.production for those common for all environments. Use the .env.bratiska-cli-build.<env> files for setup specific to environment.:::caution bratiska-cliToday bratiska-cli works by overriding .env.production.local file - your data stored in this file will be over-written on each build!:::","strapi-1#Strapi":"You can use .env. We don't have a way to set these up per-environment at the moment.","nestjs-1#Nest.js":"We don't have a mechanism (or at the moment a need) to have or persist these.","run-time-environment-variables#Run time environment variables":"These are the same for all frameworks and are stored in .env files in kubernetes directory of each project. There are a few options where to put this .env files, based on whether the config is global for all deployment environments, or only for some:\nuse /kubernetes/base/.env for config common for all deployments\nuse /kubernetes/envs/<Env>/.env for specific env, where <Env> is one of Dev, Prod, Staging","secrets#Secrets":"We are using Sealed Secrets https://github.com/bitnami-labs/sealed-secrets.\nTo use a secret in your project, you have to install kubeseal if you haven`t installed it yet.\nbrew install kubeseal\nAfterwards go to the folder secrets where all our secrets are stored:\ncd kubernetes/base/secrets\n:::note Sealed secret can be generated in a single step from the cliFor a simpler way of creating the secrets file, go to the Tips & Tricks section at the bottom:::After that, we need to create a temp file for our new secrets. Let's assume we want database connection secretes. You need to make this file database.yml\napiVersion: v1\nkind: Secret\nmetadata:\nname: database-secret\nannotation:\nsealedsecrets.bitnami.com/managed: \"true\"\ndata:\nPOSTGRES_DB: YmFuYW5h\nPOSTGRES_USER: YmFuYW5h\nPOSTGRES_PASSWORD: YmFuYW5h\n\nmetadata.name is the name of the group of secrets in our case, database-secret - if this is app specific, it is often prefix by app name, so for example bratislava-strapi-database-secret\nannotation/sealedsecrets.bitnami.com automatically creates \"unsealed\" secret inside k8 cluster, managed by the bitnami secret plugin\ndata contains environment variables keys (POSTGRES_DB) and base64 encode values (YmFuYW5h).\n\nFor example, if you need to set up the database name to banana, you need to base64 encode this value. You can use an online base64 converter like https://www.base64encode.org and encode banana to YmFuYW5h. This has to happen even if the value you want to provide is base64 encoded! In such case you'll take your base64 encoded value and encode it again.The last thing is encrypting our secrets by kubeseal to be used on Kubernetes. You need to run this command that creates the file database.secret.yml where all our values are encrypted and safe to add to the repository.Before running this command be sure you are logged in right cluster kubectl config use-context tkg-innov-<env> (replace <env> with one of dev, staging or prod. Cluster you are logged in is used when generating secret. Regarding to this, if you are generating secret for more then one cluster you need to switch between clusters between each generation of secret.\nkubeseal --controller-name=sealed-secrets --scope=namespace-wide --namespace=standalone --format=yaml < database.yml > database.secret.yml\nIf you want to propagate a sealed secret to Kubernetes without a pipeline, you can run this command:\nkubectl create -f database.secret.yml\nIf you already have a sealed secret in Kubernetes, you can update it with the command:\nkubectl apply -f database.secret.yml\nUsually, you get this kind of error: Error from server (AlreadyExists): error when creating \"database.secret.yml\": sealedsecrets.bitnami.com \"nest-Prisma-template-database-secret\" already existsIf you want to check if your secret is there, you can run this command:\nkubectl get secret --namespace=standalone nest-prisma-template-database-secret\nAfter creating file with secret using previous commands, we need to erase previously added secret which shouldn't leak to end users (or be placed under source control).That's why we edit file including secret, in our case database.yml. Usually, we replace it with <replace-with-base64-password>. So before committing into source control, file should look like this:\napiVersion: v1\nkind: Secret\nmetadata:\nname: database-secret\nannotation:\nsealedsecrets.bitnami.com/managed: \"true\"\ndata:\nPOSTGRES_DB: <replace-with-base64-password>\nPOSTGRES_USER: <replace-with-base64-password>\nPOSTGRES_PASSWORD: <replace-with-base64-password>\nTo use this secret in k8 deployment you need to add secret name from file database.yml property metadata.name in our case, database-secret to kubernetes/base/deployment.yml into\nspec:\ntemplate:\nspec:\ncontainers:\n- image: ${IMAGE_TAG}\nenvFrom:\n- secretRef:\nname: ${BUILD_REPOSITORY_NAME}-database-secret","database-naming-convention#Database naming convention":"Please use our services names (project-slugs) as database names and users. In this case, we will use nest-prisma-template. And for passwords, use at least 16 characters long pass with random chars.\nPOSTGRES_DB: nest-prisma-template\nPOSTGRES_USER: nest-prisma-template\nPOSTGRES_PASSWORD: LBcdso08b&aasd(ck2*d!p\nwhich after base64 encoding looks like this:\nPOSTGRES_DB: bmVzdC1wcmlzbWEtdGVtcGxhdGU=\nPOSTGRES_USER: bmVzdC1wcmlzbWEtdGVtcGxhdGU=\nPOSTGRES_PASSWORD: TEJjZHNvMDhiJmFhc2QoY2syKmQhcA==","tips--tricks#Tips & Tricks":"If you don't need special settings for your secret, you can create entire kubesealed secret by running following command:\nkubectl create secret generic <SECRET_NAME> --from-literal=<KEY>=<VALUE> --dry-run=client -o json \\\n| jq '. += { \"annotation\": {\"sealedsecrets.bitnami.com/managed\": \"true\"} }' \\\n| jq '.metadata += { \"labels\": {\"app\": \"${BUILD_REPOSITORY_NAME}\", \"source\": \"${BUILD_REPOSITORY_NAME}\"} }' `# this will add bratiska-cli build labels to secret` \\\n| kubeseal --controller-name=sealed-secrets --scope=namespace-wide -o yaml --namespace=<NAMESPACE>\nSticking with our banana example, we create a database-secret with \"banana\" user, password and DB in namespace \"standalone\" and push it to the file name database.secret.yml:\nkubectl create secret generic database-secret \\\n--from-literal=POSTGRES_DB=banana \\\n--from-literal=POSTGRES_USER=banana \\\n--from-literal=POSTGRES_PASSWORD=banana \\\n--dry-run=client -o json \\\n| jq '. += { \"annotation\": {\"sealedsecrets.bitnami.com/managed\": \"true\"} }' \\\n| jq '.metadata += { \"labels\": {\"app\": \"${BUILD_REPOSITORY_NAME}\", \"source\": \"${BUILD_REPOSITORY_NAME}\"} }' \\\n| kubeseal --controller-name=sealed-secrets --scope=namespace-wide -o yaml --namespace=standalone > database.secret.yml\nNote, you may need to install jq by standard means like\nbrew install jq\nor Debian based\napt install jq","env-vars-cookbook#Env vars cookbook":"Use the first that applies for the framework you are using","nextjs-2#Next.js":"if it's local development only value, that can be public to the world, store it in .env.development\nif it's local development value that can't be committed / shared with the world, put a placeholder into .env.development guiding devs to ask for this value (i.e. SECRET_VAR=<get-this-from-@user>) - store the value itself in .env.development.local which is gitignored\nif it's public information and/or build-time variable common to all deployments store it in .env.production file (committed to git) - anything prefixed with NEXT_PUBLIC_ common to all deployments is here\nif it's public information and/or build-time variable different across deployments use one (or all) of the files: .env.bratiska-cli-build.dev, .env.bratiska-cli-build.staging, .env.bratiska-cli-build.prod\nif it shouldn't be stored in git / viewed by public use kubeseal and create a sealed secret - this can be common for all environments in a file like kubernetes/base/secrets/your-secret.secret.all.yml or specific for each one i.e. kubernetes/base/secrets/your-secret.secret.staging.yml - see the appropriate section in Secrets on how to create these\n\nWhen in doubt, use the Next.js environment variable order guide.","strapi-2#Strapi":"if it's local development only value, that can be public to the world, store it in .env.example - during development these values should be copied to .env.local\nif it's local development value that can't be committed / shared with the world, put a placeholder into .env.example guiding devs to ask for this value (i.e. SECRET_VAR=<get-this-from-@user>), store the value itself in .env.local\nif it's public information and/or build-time variable common to all environments store it in .env file (committed to git)\nif it's public information different across deployments use one (or all) of the files: /kubernetes/envs/Dev/.env, /kubernetes/envs/Staging/.env, /kubernetes/envs/Prod/.env\nif it shouldn't be stored in git / viewed by public use kubeseal and create a sealed secret - this can be common for all environments in a file like kubernetes/base/secrets/your-secret.secret.all.yml or specific for each one i.e. kubernetes/base/secrets/your-secret.secret.staging.yml - see the appropriate section in Secrets on how to create these","nestjs-2#Nest.js":"if it's local development only value, that can be public to the world, store it in .env.example - during development these values should be copied to .env.local\nif it's local development value that can't be committed / shared with the world, put a placeholder into .env.example guiding devs to ask for this value (i.e. SECRET_VAR=<get-this-from-@user>), store the value itself in .env.local\nif it's public information same for all deployments use /kubernetes/base/.env\nif it's public information different across deployments use one (or all) of the files: /kubernetes/envs/Dev/.env, /kubernetes/envs/Staging/.env, /kubernetes/envs/Prod/.env\nif it shouldn't be stored in git / viewed by public use kubeseal and create a sealed secret - this can be common for all environments in a file like kubernetes/base/secrets/your-secret.secret.all.yml or specific for each one i.e. kubernetes/base/secrets/your-secret.secret.staging.yml - see the appropriate section in Secrets on how to create these"}},"/recipes/alerting/contact-point":{"title":"Contact Point","data":{"":"In this recipe we discuss how to setup a new contact point, so you can better target your alert notification to relevant contacts.To whom Grafana sends the alerts is driven by contact points. You can find the setting in \"Alerting\" (Bell icon on the left menu) section.Currently, there are two ways Grafana can send you a alert notification.\nA Grafana Bratislava Slack application/bot, that you can add to your channel or create a chat directly\nEmail address grafana[at]devops.bratislava.sk, that you can use to send a alert notification to you mailbox\n\nThe default setting is to send alerts to Slack Bratislava Innovation Grafana alerting channel. This should serve for most use-case and if you do not wish to change this, you don't need any additional setup when creating your own alerts.Note, that not all users can add contact points, you need to be Grafana administrator. If you can't access these notification settings, please ask someone from DevOps to add it for you.","add-new-slack-channel-or-chat#Add new Slack channel or chat":"To add a new Slack channel or chat as a contact point, we need to first add Grafana Slack App into the channel/chat.To add Grafana App to the channel\nWe need to open a channel settings and click Integrations\nThere you will see \"Add an App\" button. When clicked, a list of all available application will show up\nWe need to select correct Grafana application. Our application has a description \"Grafana @ grafana.bratislava.sk application\"\n\nTo be able to receive alerts in a chat\nYou just need to actually start a chat with Grafana App\nYou can start it as you would start any other chat with any other person and just type into \"To:\" field \"Grafana\"\n\nNote, that the application is only part of Bratislava Innovation group and your channel needs to be part of it also.If you have successfully integrated Grafana into your channel/chat, then it is rest is very straight forward.\nIn Grafana's notification settings click \"+ New contact point\" button\nGive it a name, like \"Slack - <name of the channel>\" or \"Slack - <name of the person>\" in case of a chat, e.g., \"Slack - DevOps\" or \"Slack - jozko.mrkvicka\"\nSelect a contact point type to \"Slack\"\nSpecify recipient, by either giving the name of the channel or name of the person in case of chat or respective Slack IDs. You can find the Slack ID in the channel/chat settings.\nGive it a Bot token. If you are administrator of the App you can find the token in Grafana App Settings, if not, ask somebody from DevOps to provide a token for you.\n\nAnd that's it. You have successfully added a new contact point into Grafana, you can test it by hitting \"Test\" button.If you want to receive specific alerts on this channel you will also need to create a notification policy to match those alerts. You can look at \"Link alerts with a contact point\" section.","add-new-email-address#Add new email address":"Grafana email alerting is setup through our mailgun smtp server with address grafana[at]devops.bratislava.sk.To add an email contact point, we need to\nVisit Grafana alerting notification settings\nClick \"+ New contact point\" button\nGive it a name, like \"Email - <address>\" or \"Email - <project>\"\nSpecify email addresses to which to send notifications\n\nAnd that's it. Grafana should be sending alert notification, you can test it by hitting \"Test\" button.If you want to receive specific alerts on this email you will also need to create a notification policy to match those alerts. You can look at \"Link alerts with a contact point\" section.","link-alerts-with-a-contact-point#Link alerts with a Contact Point":"Setting up a contact point is just a half of the work. If you want to send specific alerts to specific contacts, you need to also setup a \"Notification Policy\". You can find it in alerting section (Bell icon on the left menu), under \"Notification policies\".To build a notification policy we need to\nFirst hit the \"+ New notification policy\" button\nAdd a matcher. This will create rules, based on which we will \"match\" or link specific alerts with your contact point\nHit \"+ Add matcher\" button\nAdd as many label conditions as you need, to match only that group of alerts that you need\n\n\nSelect a contact point\n(Optional) You can also setup a \"Mute Timing\" or if there are already some, you can choose one. This will \"mute\" alert notification based on specified conditions. For example, you might want to receive alerts only during your working hours, so you would create a mute timing that would pause notifications during your off time.\n\nAnd that's it. Now you have linked your contact point with your policy and should be receiving alert notifications."}},"/recipes/alerting/endpoint_alert":{"title":"Endpoint Alert","data":{"":"Sometime you want to monitor endpoint that your application exposes, such as various health endpoints and/or the data those endpoints provide. For this, use-case we use Infinity data source that provides simple capabilities on making requests to endpoints and parsing the JSON response data.To create such an alert we need to\nFirst go to Alert section in Grafana (bell icon on the left menu). Then hit the \"+ New alert rule\" button.\n\n\n\nFill in the metadata of rule\nRule name, give it what ever you feel is descriptive\nFolder, select based on the cluster\nGroup, you can put anything in place of a group, like project name\nBear in mind that all alerts within the same group will be evaluated at the same time. So, if you are planing on creating more alerts for one project, we would suggest to give it the name of that project\n\n\nSelect \"Infinity\" as data source. Optionally, we can also change name of expression from \"A\" to maybe something more descriptive\nThe only thing we need to specify is Parser field to be \"Backend\". Also note, that for alerting, Infinity supports only JSON queries\nThen just fill out the URL. Select method that you want (GET, POST) and fields to parse out from the JSON response based on what you want to check\n\n\nNote, Grafana can only work and alert on numeric responses, therefore your endpoint has to return some sort of a number\nNext, we need to add \"Math\" expression where we check out whether response is correct. In our example case, we want alert if response status is not 200\n\n\n\nWe need to define alert condition. Select the name of our \"Math\" expression and set check interval (Evaluate) and for how long should the alert be pending before firing (for).\nLastly, we want to add some description and summary to our alert, with some custom labels, that would help us to better specify what is going on, when we receive the alert notification\n\n\n\n\nAnd that is it. Now you can just \"Save and exit\", and your alert should be running, and firing in case of any issues.\nThe default contact point is through Slack to grafana-alerting channel. If you want to receive your alerts somewhere else or through some other means, please checkout \"How to add Contact Point\" recipe.","examples#Examples":"Infinity alert complex health check on application endpoint"}},"/recipes/alerting/grafana_alerting":{"title":"Grafana Alerting","data":{"":"In recipes for this section, we will discuss\nHow to setup a new alert for your application\nHow to link alerts with specific contact points for better relevant targeting ⇲.\n\nWe, somewhat arbitrary, split the alert into 3 different categories\nAlerting on application system resources (CPU, Memory, Disk, etc.) ⇲\nAlerting on application's logs and specific keywords or pattern in those logs ⇲\nAlerting on availability of specific endpoints or data provided by those endpoints ⇲\n\nThis categorization is in line with the supporting Grafana applications/data sources, that all have different use-cases and all need a bit different syntax. Currently we support\nPrometheus is a monitoring tool that sits on top of our kubernetes infrastructure and provides various metrics about nodes and application health, resources, etc.\nLoki is a Grafana application that specializes in logs monitoring and alerting.\nInfinity is a very simple application that provides HTTP requests capabilities, it can be used to monitor health endpoint and is capable of parsing JSON responses and alerting on them\n\nAnd for more information on how all of this is setup, please visit Observability and Monitoring section."}},"/recipes/alerting/log_alert":{"title":"Log Alert","data":{"":"In this section we discuss how to build a alert on top of your application logs. As an example, we will fire a alert, when we see \"ERROR\" keyword in the logs of our application.\nTo build such a log alert we need to\nFirst go to Alert section in Grafana (bell icon on the left menu). Then hit the \"+ New alert rule\" button.\n\n\n\nFill in the metadata of rule\nRule name, give it what ever you feel is descriptive\nFolder, select based on the cluster\nGroup, you can put anything in place of a group, like project name\nBear in mind that all alerts within the same group will be evaluated at the same time. So, if you are planing on creating more alerts for one project, we would suggest to give it the name of that project or something similar\n\n\nNext, in the second section we need to change the dataset from \"Prometheus\" to \"Loki\". Optionally, we can also change name of expression from \"A\" to maybe something more descriptive.\nWe also need to change the time span on which we are going to alert on. We need to select \"now-1h to now\", other options are not available or not work well in the current Grafana implementation (Issue #48913)\n\n\n\nNow for the hard part, we need to build Loki query. The query language can do a lot of things, but for monitoring on simple phrases we just need a simple expression that selects our application within {}, wrapped in count_over_time function.\ncount_over_time({cluster=\"tkg-innov-prod\", namespace=\"standalone\", app=\"example-app\"} |= \"ERROR\" [10m])\nAfter {} selection, we have |= operator, which matches exactly the expression \"ERROR\". But, we can also do |~ operator, which is capable matching on regex expressions.\nThis expression select logs only for our specific application then matches the string \"ERROR\", and over time of 10 minutes counts the number of occurrences of that string.\nNote, that you can use \"Explore\" (compass icon in left menu) UI to build and test your queries.\nThen the process is very similar to the one that is describe in \"Alert on resources\" section, steps 7. onwards. We need to create two more expressions, one with Operation \"Reduce\" and the next with Operation \"Math\".\nFor \"Reduce\", we want to select \"Last\" function, which reduces our query to last know value\nFor \"Math\", we want to put our math expression, in our case,\nwe want to alert on any one occurrence of \"ERROR\". Meaning, $last_count_log >= 1 (notice, we renamed our \"Reduce\" expression to last_count_log, for better readability)\n\n\n\n\n\nWe need define alert condition. Select the name of our \"Math\" expression and set check interval (Evaluate). Also, set for how long should the alert be pending before firing (for).\nLastly, we give the alert description and summary, and if needed we put specific labels to help us quickly understand which error is firing when we get a notification.\n\n\n\nNote, you can use template variables to customize your alert details, as seen on the picture above.\n\nAnd that is it. Now you can just \"Save and exit\", and your alert should be running, and firing in case of any issues.\nThe default contact point is through Slack to grafana-alerting channel. If you want to receive your alerts somewhere else or through some other means, please checkout \"How to add Contact Point\" recipe.","examples#Examples":"Log alert on specific keyword. This is the example alert for one of our application, that fires every time we see the \"ERROR\" string in the logs of said application."}},"/recipes/alerting/resource_alert":{"title":"Resource Alert","data":{"":"Our applications will inherently have some limits, for instance, memory (RAM) or disk and it will probably be useful to alert in case, disk reaches 80% capacity. For this, use-case we use Prometheus data source that captures these metrics for individual instance spined up in our cluster.The easiest way to create a alert on resource utilization is to visit any of our Grafana dashboard, that contains information you want to alert on. We recommend Pod Dashboard, in case of alerting on app/pod/container and will use it in following example: Alert on application high memory utilization.So, to setup an alert, let's say bratislava-next-app on prod cluster, when reaches 90% memory utilization, we need to\nSet the filters, on top of the page to the desired output. In our case it would be\n\n\n\nChoose the panel that contains the information that we want alert on. Click on the name of the panel and hit \"edit\"\n\n\n\nThere, you will see already prepared query, that will contain your information (cluster, pod, etc.). Select \"Alert\" in the menu\n\n\n\nNow, you should see all existing alerts tied to the panel/chart. Hit the \"Create alert rule from this panel\" button\nIf you get a message saying, to save the dashboard, hit Discard\n\nThis will open up a alert UI, where your query should be already filled in, with correct values from your filters.\nYou can skip steps above if you know Prometheus query language and just go directly to this UI through alert button in the left menu (bell icon) and fill in the query by your self.\nFill in the values, for\nRule name, give it what ever you feel is descriptive\nFolder, select based on the cluster\nGroup, you can put anything in place of a group, like project name\nBear in mind that all alerts within the same group will be evaluated at the same time. So, if you are planing on creating more alerts for one project, we would suggest to give it the name of that project\n\n\n\n\n\nNow, we need to construct the alert expression. There should be already a expression called A, with selected Operation field \"Classic condition\". Let's change that to \"Reduce\" from the drop down menu\nAs Function let's put \"Last\" and as Input select the query name above. This reduces the entire series only to the last know value. You can also choose, what to do with missing values, for example, replace them with another specific value or keep them as NaN\n\n\n\nClick \"+ Expression\" button\nThis should create a B expression. Let's select as Operation, option \"Math\", which will create a big text box where you can write math expression\nYou can use the query names as variables, so in our case we want to evaluate when expression $A is higher or equal to 90. Therefore, we write $A >= 90\n\n\n\n\nThat is almost it, we only need to setup alert condition.\nIn a Condition field, we need to pick our \"Math\" expression name, in our case \"B\"\nChoose evaluation interval and for how long we want the alert to be in a \"Pending\" state before it starts firing\nIf you are wondering where this is useful, it might be that in some case you have very volatile metric, like CPU utilization. This can go rapidly up and then in a minute go again down to normal levels. You might not want alert every time this happens, but put the alert in a \"Pending\" state and check latter if the metric is still in breach of the condition or it is back \"Normal\"\nYou can also select what you want to do in case of missing data\n\n\n\n\nNow, we are done with the alert and you can \"Save and exit\", and the alert would start running. But it might be beneficial to put some more information about it. You can do it the next section and put there some Description and Summary.\n\nAlso, put some other specific labels, which you can also then use to specify in a contact point to target your alerts. Something like\n\n\nNote, that by default any labels from your query will be also assign to your alert, for instance, stuff like app, pod, cluster, etc. You can use these as variables in your summary and description through templates. This is useful when your query encapsulates multiple applications, you can use these variables to specify for which application the alert if firing.And that is it. Now you can just \"Save and exit\", and your alert should be running, and firing in case of any issues.\nThe default contact point is through Slack to grafana-alerting channel. If you want to receive your alerts somewhere else or through some other means, please checkout \"How to add Contact Point\" recipe.","examples#Examples":"You can take a look on following examples of application and kubernetes node resource alerting\nPrometheus alert on application resources. It checks all deployed application on prod cluster if their long term memory usage is >= 90%\nPrometheus alert on kubernetes node resources. Checks long term memory usage of kubernetes worker nodes, if their utilization is >= 90%"}},"/strapi/general-concepts":{"title":"General Concepts","data":{"":"We use Strapi as our Headless CMS, from which we pull data into Next.js frontend via GraphQL.If you are not a developer and are interested primarily in managing content within Strapi, this Strapi user guide on Content Manager is a good start.The rest of this section will talk about the basic concepts of modelling our data within Strapi and the way we pull it into our frontend applications.","basic-concepts-of-a-headless-cms#Basic concepts of a headless CMS":"A headless CMS like Strapi, as opposed to a 'regular' CMS like Wordpress, does not send us a finished, styled webpages, but sends only raw data to our frontend (our Next.js application) - based on the format and contents of this data we decide what to render.You can read more in this Strapi articleLet's distinguish two use-cases from our page ecosystem to illustrate this.","template-content---pages--entities-which-always-display-the-same-data-model#\"Template\" content - pages & entities which always display the same data model":"A straightforward application of the Headless CMS concept is when we have a template page on our frontend and we just fill it with data coming from Strapi. An example of this are \"Events\" from city-library.\n\nEach of the Event pages contains a cover image, date, title, a rich-text field for text detail, and a few other fields. To implement this, Event is first created as a content-type model within our Strapi server and populated with all the fields we may require (again, refer to Strapi docs). Instances of the Event model are then pulled into our frontend app using GraphQL.For more info on GraphQL, getting the GraphQL data into React and browsing the gql queries see the appropriate section.In this use-case, data is always 'consumed' by the same React component(s).","dynamic-content-with-strapi-cms#Dynamic content with Strapi CMS":"Sometimes we want a more 'page-builder' like experience as a Strapi administrator - we may want to add various different React components (think rich-text, tables, galleries, accordions...) to the page or article we are writing, and have control over the order in which they appear. To do this, we are using Strapi components - not to be confused with React components, these are simply reusable data models or in other words combinations of several Strapi data fields - which we then place into Dynamic Zones within their parent content-type. You can see how Dynamic Zones are fetched with GraphQL here.On React's side, we then need to map each Strapi Component instance to an instance of a React component which can render it's data. There is usually a one-to-one mapping between the __typename field fetched via gql and a React component. A good example of this is the Sections.tsx file from bratislava.sk page:\nconst SectionContent = ({ section, locale }: { section: SectionsFragment; locale?: string }) => {\nconst { t } = useTranslation(\"common\")\nswitch (section.__typename) {\ncase \"ComponentSectionsNarrowText\":\nreturn (\n<NarrowText\nalign={section.align ?? undefined}\ncontent={section.content ?? undefined}\nwidth={section.width ?? undefined}\nhasBackground={section.hasBackground ?? false}\n/>\n)\n\ncase \"ComponentSectionsIconTitleDesc\":\nreturn (\n<RentBenefits\ntitle={section.title}\nlist={section.list}\nlinkLabel={t(\"readMore\")}\nhasBackground={section.hasBackground ?? false}\n/>\n)\n\ncase \"ComponentSectionsDocumentList\":\nreturn <DocumentList />\n\ncase \"ComponentSectionsColumnedText\":\nreturn (\n<ColumnedText\ncontent={section.content ?? \"\"}\nhasBackground={section.hasBackground ?? false}\n/>\n)\n// ... and many more sections cases\n}\n}\n\nconst Sections = ({\nsections,\nlocale,\n}: {\nsections: (SectionsFragment | null)[]\nlocale?: string\n}) => {\nreturn (\n<>\n{sections.map((section, index) => (\n// eslint-disable-next-line react/no-array-index-key\n<SectionContent key={index} section={section} locale={locale} />\n))}\n</>\n)\n}\n\nexport default Sections\nMapping the data this way, we'll get our page-builder experience - the CMS admins being able to add new dynamic React components (from a pre-defined set) to their articles by adding the matching Strapi component to lists of dynamic zones within Strapi.","multiple-dynamic-sections#Multiple dynamic sections":"You may require multiple 'dynamic' sections within your content-types. I.e. Page model in bratislava.sk consists of PageHeaderSections and Sections - the first one is content rendered in Page Header, the other within the 'body' of the page. Page header has it's own set of components which works within this context and is distinct from the components rendered in the body.","sharing-the-same-components-within-multiple-content-types#Sharing the same components within multiple content-Types":"The above does not mean that for each dynamic section the set of Strapi components need to be distinct. I.e. the body of Pages and BlogPosts consists of the same set of elements, and subsequentially, the code for the two renders the same set of React components and their respective content-types allow for same set of Strapi Components in their Sections dynamic zone.","creating-content-types-and-displaying-it-on-frontend#Creating content-types and displaying it on frontend":"The steps when adding a completely new entity model are roughly as follows:\ncreate a content-type within Strapi - good introduction here, project specifics within the docs of each of our projects as needed. Creating content-types modifies the code, therefore you need to do this from a locally running instance, and commit the changes into git.\nwrite the GraphQL query and generate code which fetches it with GraphQL Codegen - see our GraphQL docs(TODO)\nfetching the data in Next.js and displaying it within our frontend 'template' - see our GraphQL docs(TODO)\n\nFor a more concrete example check out the guide for adding sections to bratislava.sk - note that specifics may vary slightly from project to project."}},"/strapi/load-strapi-db-in-local-dev":{"title":"Load Strapi Db in Local Dev","data":{"":"You'll need access to our Azure Devops project o access the database dumpsYou can then use the db-backup pipeline to browse recent backups on different k8s deployed projects. Each run produces an artifact in which you'll find the db dump.In all likelihood, you'll be interested in the runs titled <Env>_standalone, where <Env> is the k8s cluster you are after.Assuming you have:\npostgres server running locally & psql cli tool installed\na user/role called strapi\na database your_strapi_db\n\nyou can load the dump newest-db-dump.sql like so:\npsql -h localhost -p 5432 -U strapi your_strapi_db < newest-db-dump.sql\n\nIt's not always necessary, but recommended, to have a role called strapi on your local Postgres server when doing this with a Strapi db dump - as the tables may reference role with this name directly. Alternative might be editing the dump and renaming the role referenced to one you have available locally.\nTo login into the db dumped this way, you may need to ask for access from the project lead.\nSee Sync Strapi database to different environment for guide on dumping live database directly from the server, instead of relying on the regular db dumps in pipeline."}},"/strapi/setup":{"title":"Setup","data":{"":"This guide focuses on specific updates to the Strapi setup related to our own needs and requirements. For more general information about Strapi, please refer to the official documentation.","making-content-accessible-without-authentication#Making content accessible without authentication":"This needs to be done:\nwhen you setup a new instance or database, either locally or on a new deployment\nevery time you add a new content type, on every deployment\n\nGo to Settings -> in the sidebar USERS & PERMISSIONS PLUGIN: Roles -> edit icon ✏️ next to the Public role -> Permissions. In all of your custom content types, particularly the newly added ones, make sure the following permissions are checked:\nfind\nfindOne (if available)","one-time-production-setup#One-time production setup":"The steps below are needed to make our strapi instance deployment (and production) ready.","env_path-setup#ENV_PATH setup":"Strapi takes ENV_PATH environment variable to decide which .env file it should use - it's best to override the \"scripts\" section of package.json. To make it work under both Windows and Unix-like systems, install cross-env beforehand:\nyarn add -D cross-env\nThen edit package.json as follows:\n...\n\"scripts\": {\n\"develop\": \"cross-env ENV_PATH='./.env.local' strapi develop\",\n\"start\": \"strapi start\",\n\"build\": \"strapi build\",\n\"strapi\": \"strapi\",\n...\n},\n...\n\nThis will allow you to easily maintain different dev and prod env vars inside the git repo - we usually keep .env.local outside of git and commit env.development file, which the devs are encouraged to copy into their own .env.local which they can override. This system is not perfect, but seems reasonable enough in a system like Strapi which, by default, accepts only a single .env config file at once.An example of .env.local file from bratislava.sk project:\n\nHOST=0.0.0.0\nPORT=1337\nAPP_KEYS=VivFhCHdok6Ui4H1yhF8DA==,fkXjik+gn+fRLqatGAE8QQ==,M8eRp3VNi4dzdfHOTBlT7w==,PODWh8urxrSZKWXWxiEV3w==\nAPI_TOKEN_SALT=ilrHJDUcrKxGMpxyBh32VA==\nADMIN_JWT_SECRET=odOY9wudFDnl//bE+nVWAQ==\nJWT_SECRET=U4RUqp1ohkwtt6gRopMtdg==\n\nDATABASE_HOST=localhost\nDATABASE_PORT=5432\nDATABASE_NAME=strapi\nDATABASE_USERNAME=strapi\nDATABASE_PASSWORD=password\nDATABASE_SCHEMA=public","production-jwt-secrets#Production JWT Secrets":"As of Strapi v4.3 (and unlikely to change in near future), the 'clean' Strapi init generates a few secrets for you into .env file - these are used as seeds to generate one-time tokens and thus the ones used in production shouldn't be stored in github.This means you need to insert the following env vars as secrets:\n\nADMIN_JWT_SECRET\nAPI_TOKEN_SALT\nAPP_KEYS\nJWT_SECRET\n\nwhere ADMIN_JWT_SECRET are 4 base64 encoded strings separated by comma , character, and the rest are base64 encoded strings. Strapi docs recommend generating the secrets with openssl command:openssl rand -base64 32Specifically, for APP_KEYS:\nAPP_KEYS=$(openssl rand -base64 32),$(openssl rand -base64 32),$(openssl rand -base64 32),$(openssl rand -base64 32)\nThis means the secret file will look something like this (replace the placeholder values with the openssl result encoded once more into base64 - more details in secrets docs)):\napiVersion: v1\nkind: Secret\nmetadata:\nname: bratislava-strapi-internals-secret\nannotation:\nsealedsecrets.bitnami.com/managed: \"true\"\ndata:\nADMIN_JWT_SECRET: YmFuYW5h\nAPI_TOKEN_SALT: YmFuYW5h\nAPP_KEYS: YmFuYW5h\nJWT_SECRET: YmFuYW5h","other-production-setup-files#Other 'production setup' files":"Other than that, your setup should include:\nDockerfile\ndockerignore\nkubernetes directory\n\nbest copied over from one of the maintained project (i.e. bratislava.sk)"}},"/strapi/sync-strapi-db-to-different-env":{"title":"Sync Strapi Db to Different Env","data":{"":"Below is a set of commands to copy production Strapi data to staging environment - can be modified for other similar tasks. If you need exact copy of database, you should scale down strapi & next deployments and drop & recreate database before loading the data.\n# below, update user@bratislava.sk, user.name and optionally strapi-backup.sql to unique file name\n\n# recommended to do step-by-step\n# skip the first step if on Magistrate network or already on vpn, skip the second if already logged in to kubernetes clusters\n\n# Step 1 - vpn\n\nsudo openfortivpn vpn.bratislava.sk:443 --username=user.name --trusted-cert 249c03e8a78ee9b45b9f2afa2e13bd59da1384b7377d133fa0caff86af45b28d\n\n# Step 2 - kubectl login\n\nkubectl vsphere login --server=10.10.10.1 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name=tkg-innov-staging -u user@bratislava.sk\nkubectl vsphere login --server=10.10.10.1 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name=tkg-innov-prod -u user@bratislava.sk\n\n# Step 3 - download dump\n\nkubectl config use-context tkg-innov-prod\nkubectl exec -t -n standalone bratislava-strapi-database-0 -c database -- sh -c \"pg_dump -c -U strapi strapi > strapi-backup.sql\"\nkubectl cp standalone/bratislava-strapi-database-0:/strapi-backup.sql ./strapi-backup.sql\nkubectl config use-context tkg-innov-staging\n\n# Checkpoint - recheck that you are in correct context!\n\nkubectl config get-contexts\n\n# Step 4 - import dump\n\nkubectl cp ./strapi-backup.sql standalone/bratislava-strapi-database-0:/strapi-backup.sql\nkubectl exec -t -n standalone bratislava-strapi-database-0 -c database -- sh -c \"psql -U strapi strapi < strapi-backup.sql\""}}}